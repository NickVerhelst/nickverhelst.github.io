{"config":{"indexing":"full","lang":["en"],"min_search_length":2,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Home Welcome to my very personal documentation page. At this site I collect my personal notes on software design, code snippets and tips and tricks that I personally find useful. This website started out as a random collection of code snippets and good ideas and kept on growing beyond. It is still a growing work in progress. If you find that something is missing and/or wrong, feel free to let me know! A big shoutout to mkdocs. This package was (and still is) used to build this web page.","title":"Home"},{"location":"#home","text":"Welcome to my very personal documentation page. At this site I collect my personal notes on software design, code snippets and tips and tricks that I personally find useful. This website started out as a random collection of code snippets and good ideas and kept on growing beyond. It is still a growing work in progress. If you find that something is missing and/or wrong, feel free to let me know! A big shoutout to mkdocs. This package was (and still is) used to build this web page.","title":"Home"},{"location":"Azure/","text":"Azure A collection of Azure code snippets. The prefix will denote the discussed service. For the prefixes, we follow the Azure Best Practices on abbreviations : ADF: Azure Data Factory","title":"Azure"},{"location":"Azure/#azure","text":"A collection of Azure code snippets. The prefix will denote the discussed service. For the prefixes, we follow the Azure Best Practices on abbreviations : ADF: Azure Data Factory","title":"Azure"},{"location":"Azure/adf-triggers/","text":"ADF - Triggers Schedule Trigger Is just a point trigger. This trigger will contain no time information (except the \"TriggerTime\"). In order to get time info, you can extract the current date using formatDateTime. @formatDateTime ( utcnow (), 'yyyyMMdd' ) Tumbling Window Trigger With the tumbling window trigger you do have time information. Next to the \"TriggerTime\", you also have access to the \"WindowStartTime\" and \"WindowEndTime\". In order to get the starting time of the window for example you could use the code below. @formatDateTime ( trigger () . outputs . windowStartTime , 'yyyyMMdd' )","title":"ADF - Triggers"},{"location":"Azure/adf-triggers/#adf-triggers","text":"","title":"ADF - Triggers"},{"location":"Azure/adf-triggers/#schedule-trigger","text":"Is just a point trigger. This trigger will contain no time information (except the \"TriggerTime\"). In order to get time info, you can extract the current date using formatDateTime. @formatDateTime ( utcnow (), 'yyyyMMdd' )","title":"Schedule Trigger"},{"location":"Azure/adf-triggers/#tumbling-window-trigger","text":"With the tumbling window trigger you do have time information. Next to the \"TriggerTime\", you also have access to the \"WindowStartTime\" and \"WindowEndTime\". In order to get the starting time of the window for example you could use the code below. @formatDateTime ( trigger () . outputs . windowStartTime , 'yyyyMMdd' )","title":"Tumbling Window Trigger"},{"location":"Azure/application-insights-queries/","text":"Application Insights Queries A collection of interesting queries to analyze logs using Application Insights. For analysis of failures there are two interesting datasets that can be used: Requests: This logs each request that is received by your services (e.g. a Function App). Traces: This dataset contains all logs that are recorded during the execution of the services. Get all failed runs from the last X days // Fetch failed operations in for a given history. // The history length is specified via the history_days argument. // To create an alert for this query, click '+ New alert rule'. // PARAMETERS let history_days = ago ( 3 d ); // QUERY requests | where timestamp > history_days | where success == false | order by timestamp desc | project timestamp , name , duration , operation_Name , operation_Id Get log messages for a specific run // Fetch logs for a specific run (e.g. a function run). // The run is specified via an 'operation_id' (added as parameter operation_id). // To create an alert for this query, click '+ New alert rule'. // PARAMETERS let operation_id = ' facc75623d7ad240a7c60acae59b7593 ' ; // QUERY union traces | union exceptions | where operation_Id == operation_id | order by timestamp asc | project timestamp , logLevel = customDimensions .[ ' LogLevel ' ], message = iff ( message != '' , message , iff ( innermostMessage != '' , innermostMessage , customDimensions .[ ' prop__ { OriginalFormat } ' ])) Filter log messages op specific content // Searches logs for a given history and looks for a given message. // The history is specified via 'history_days' and the message via 'message_content'. // To create an alert for this query, click '+ New alert rule'. // PARAMETERS let history_days = ago ( 1 d ); let message_content = ' Message to look for in the logs ' ; // QUERY union traces | union exceptions | where timestamp > history_days | where ( message contains message_content ) | order by timestamp desc | project timestamp , operation_Id , logLevel = customDimensions .[ ' LogLevel ' ], message = iff ( message != '' , message , iff ( innermostMessage != '' , innermostMessage , customDimensions .[ ' prop__ { OriginalFormat } ' ]))","title":"Application Insights Queries"},{"location":"Azure/application-insights-queries/#application-insights-queries","text":"A collection of interesting queries to analyze logs using Application Insights. For analysis of failures there are two interesting datasets that can be used: Requests: This logs each request that is received by your services (e.g. a Function App). Traces: This dataset contains all logs that are recorded during the execution of the services.","title":"Application Insights Queries"},{"location":"Azure/application-insights-queries/#get-all-failed-runs-from-the-last-x-days","text":"// Fetch failed operations in for a given history. // The history length is specified via the history_days argument. // To create an alert for this query, click '+ New alert rule'. // PARAMETERS let history_days = ago ( 3 d ); // QUERY requests | where timestamp > history_days | where success == false | order by timestamp desc | project timestamp , name , duration , operation_Name , operation_Id","title":"Get all failed runs from the last X days"},{"location":"Azure/application-insights-queries/#get-log-messages-for-a-specific-run","text":"// Fetch logs for a specific run (e.g. a function run). // The run is specified via an 'operation_id' (added as parameter operation_id). // To create an alert for this query, click '+ New alert rule'. // PARAMETERS let operation_id = ' facc75623d7ad240a7c60acae59b7593 ' ; // QUERY union traces | union exceptions | where operation_Id == operation_id | order by timestamp asc | project timestamp , logLevel = customDimensions .[ ' LogLevel ' ], message = iff ( message != '' , message , iff ( innermostMessage != '' , innermostMessage , customDimensions .[ ' prop__ { OriginalFormat } ' ]))","title":"Get log messages for a specific run"},{"location":"Azure/application-insights-queries/#filter-log-messages-op-specific-content","text":"// Searches logs for a given history and looks for a given message. // The history is specified via 'history_days' and the message via 'message_content'. // To create an alert for this query, click '+ New alert rule'. // PARAMETERS let history_days = ago ( 1 d ); let message_content = ' Message to look for in the logs ' ; // QUERY union traces | union exceptions | where timestamp > history_days | where ( message contains message_content ) | order by timestamp desc | project timestamp , operation_Id , logLevel = customDimensions .[ ' LogLevel ' ], message = iff ( message != '' , message , iff ( innermostMessage != '' , innermostMessage , customDimensions .[ ' prop__ { OriginalFormat } ' ]))","title":"Filter log messages op specific content"},{"location":"Data/big-data/","text":"Big Data Definition When we talk about \"Big Data\" one typically thinks of the 4 V's (dimensions) of big data: Volume: size of the data. Velocity: Speed at which the data is generated. With IoT devices, this is only increasing. Variety: More types of data are emerging, next to the classical SQL database (structured data), we are now also getting semi-structured datasets (e.g. NoSQL and graph databases) and even unstructured data (e.g. images and videos). Veracity: the quality of the data. With more and more data coming in, it is also harder and harder to control the data quality. From 'small data' to 'big data' Typically you can think of the step from \"small\" to \"big\" as the moment at which it is no longer possible to process the data on a single machine. This can be with datasets of several hundreds of gigabytes, think of large SQL tables that you can smartly index and split up so that it can be easily processed locally. But this can already be as soon as a few gigabytes, for example when working with large images/videos that need to be processed as one whole. Note with the latter example that Data Science projects quickly tend to fall in the \"Big Data\" category as these will typically try to combine several types of data sources. For this reason, a fair amount of preprocessing is pushed to Data Engineers who will specialize in this type of processing. Big Data Technologies Spark and Hadoop Data Storage and the Data Lakehouse Evolution of the data lake (see OneNote notes). Big Data Techniques Sharding See notes DP-200.","title":"Big Data"},{"location":"Data/big-data/#big-data","text":"","title":"Big Data"},{"location":"Data/big-data/#definition","text":"When we talk about \"Big Data\" one typically thinks of the 4 V's (dimensions) of big data: Volume: size of the data. Velocity: Speed at which the data is generated. With IoT devices, this is only increasing. Variety: More types of data are emerging, next to the classical SQL database (structured data), we are now also getting semi-structured datasets (e.g. NoSQL and graph databases) and even unstructured data (e.g. images and videos). Veracity: the quality of the data. With more and more data coming in, it is also harder and harder to control the data quality. From 'small data' to 'big data' Typically you can think of the step from \"small\" to \"big\" as the moment at which it is no longer possible to process the data on a single machine. This can be with datasets of several hundreds of gigabytes, think of large SQL tables that you can smartly index and split up so that it can be easily processed locally. But this can already be as soon as a few gigabytes, for example when working with large images/videos that need to be processed as one whole. Note with the latter example that Data Science projects quickly tend to fall in the \"Big Data\" category as these will typically try to combine several types of data sources. For this reason, a fair amount of preprocessing is pushed to Data Engineers who will specialize in this type of processing.","title":"Definition"},{"location":"Data/big-data/#big-data-technologies","text":"","title":"Big Data Technologies"},{"location":"Data/big-data/#spark-and-hadoop","text":"","title":"Spark and Hadoop"},{"location":"Data/big-data/#data-storage-and-the-data-lakehouse","text":"Evolution of the data lake (see OneNote notes).","title":"Data Storage and the Data Lakehouse"},{"location":"Data/big-data/#big-data-techniques","text":"","title":"Big Data Techniques"},{"location":"Data/big-data/#sharding","text":"See notes DP-200.","title":"Sharding"},{"location":"Data/data-landscape-roles/","text":"Roles in the data landscape Technical roles Data Analyst Data Scientist Data Engineer Machine Learning Engineer/Developer Machine Learning Operations Engineer Data Architect Business roles Chief Data officer (CDO) Data Steward Data Strategist","title":"Roles in the data landscape"},{"location":"Data/data-landscape-roles/#roles-in-the-data-landscape","text":"","title":"Roles in the data landscape"},{"location":"Data/data-landscape-roles/#technical-roles","text":"Data Analyst Data Scientist Data Engineer Machine Learning Engineer/Developer Machine Learning Operations Engineer Data Architect","title":"Technical roles"},{"location":"Data/data-landscape-roles/#business-roles","text":"Chief Data officer (CDO) Data Steward Data Strategist","title":"Business roles"},{"location":"Data/data-landscape-technology/","text":"Technologies python r","title":"Technologies"},{"location":"Data/data-landscape-technology/#technologies","text":"python r","title":"Technologies"},{"location":"Data/data-quality/","text":"Data Quality The Fellegi-Holt method for data cleaning","title":"Data Quality"},{"location":"Data/data-quality/#data-quality","text":"","title":"Data Quality"},{"location":"Data/data-quality/#the-fellegi-holt-method-for-data-cleaning","text":"","title":"The Fellegi-Holt method for data cleaning"},{"location":"Data/model-building-and-validation/","text":"Model building and validation Building baseline models Regression: Take the average value of the \u201ctarget\u201d y and compare with that (simplest baseline model). Classification: Take the class that occurs the most and compare with that (simplest baseline model). Next best model: a simple linear regression. The problem with this is that it can get worse if you have strong non-linear effects. A baseline model that focusses purely on the target values is therefore more advised. Metrics Many used metrics in Data Science: Precision Recall Accuracy F1 score Correlation Logloss Mean square error (MSE) Root mean square error (RMSE) Mean absolute error (MAE) Mean Average Percent Error (MAPE) (adjusted) rsquared \\(R^2\\) (or coefficient of determination) Note that when dealing with multiple classes there is also a micro-macro distinction often to be made. The score matrix Here we assign scores to outcomes. Precision-recall common curve ROC curve Calibration curve Lift curve Cumulative Gains curve Validation techniques Using the techniques and metrics above, we can now validate the different models. hyperparameter tuning, algorithm selection, how to compare models Basic parameters Bias & variance: Look at over/underfitting","title":"Model building and validation"},{"location":"Data/model-building-and-validation/#model-building-and-validation","text":"","title":"Model building and validation"},{"location":"Data/model-building-and-validation/#building-baseline-models","text":"Regression: Take the average value of the \u201ctarget\u201d y and compare with that (simplest baseline model). Classification: Take the class that occurs the most and compare with that (simplest baseline model). Next best model: a simple linear regression. The problem with this is that it can get worse if you have strong non-linear effects. A baseline model that focusses purely on the target values is therefore more advised.","title":"Building baseline models"},{"location":"Data/model-building-and-validation/#metrics","text":"Many used metrics in Data Science: Precision Recall Accuracy F1 score Correlation Logloss Mean square error (MSE) Root mean square error (RMSE) Mean absolute error (MAE) Mean Average Percent Error (MAPE) (adjusted) rsquared \\(R^2\\) (or coefficient of determination) Note that when dealing with multiple classes there is also a micro-macro distinction often to be made.","title":"Metrics"},{"location":"Data/model-building-and-validation/#the-score-matrix","text":"Here we assign scores to outcomes.","title":"The score matrix"},{"location":"Data/model-building-and-validation/#precision-recall","text":"common curve","title":"Precision-recall"},{"location":"Data/model-building-and-validation/#roc-curve","text":"","title":"ROC curve"},{"location":"Data/model-building-and-validation/#calibration-curve","text":"","title":"Calibration curve"},{"location":"Data/model-building-and-validation/#lift-curve","text":"","title":"Lift curve"},{"location":"Data/model-building-and-validation/#cumulative-gains-curve","text":"","title":"Cumulative Gains curve"},{"location":"Data/model-building-and-validation/#validation-techniques","text":"Using the techniques and metrics above, we can now validate the different models. hyperparameter tuning, algorithm selection, how to compare models","title":"Validation techniques"},{"location":"Data/model-building-and-validation/#basic-parameters","text":"Bias & variance: Look at over/underfitting","title":"Basic parameters"},{"location":"Data/resource-material/","text":"Resource material Books Sites Online courses Blogs Articles","title":"Resource material"},{"location":"Data/resource-material/#resource-material","text":"","title":"Resource material"},{"location":"Data/resource-material/#books","text":"","title":"Books"},{"location":"Data/resource-material/#sites","text":"","title":"Sites"},{"location":"Data/resource-material/#online-courses","text":"","title":"Online courses"},{"location":"Data/resource-material/#blogs","text":"","title":"Blogs"},{"location":"Data/resource-material/#articles","text":"","title":"Articles"},{"location":"Git/","text":"Git","title":"Git"},{"location":"Git/#git","text":"","title":"Git"},{"location":"Git/authentication/","text":"Authentication in Git When connecting to a repository, you will need to identify yourself and authenticate with the server. There are two ways in which the authentication can be done: Using passwordsm or using SSH keys. Password based authentication The password based authentication is fairly common since it is easy to set up. In this case you connect to the repository using your username and password. This method is not secure and therefore often not the first choice. The username and passwords are not encrypted and simply stored in plain text format. If you use this method make sure you use it on a personal computer with limited access. On Windows machines the default credential manager ( wincred ) is used to store these secrets. If this is not the case, you can set this up using the command below. git config --global credential.helper wincred On linux (WSL to be specific) there is no such default setting. When working VSCode will prompt you for credentials every 15 minutes (expiry of password). There are two options to resolve this. One method is to use the internal credential store. This can be set up using. git config --global credential.helper store An alternative is to increase the cache timeout, keeping the secrets in cache longer. The below example increases the timeout to two weeks in stead of the default 15 minutes. git config --global credential.helper 'cache --timeout==1209600' If you are using MacOS, the default credentials manager is osxkeychain. In this case you do the necessary setup using the command below. On MacOS git config --global credential.helper osxkeychain SSH Key based authentication The preferred solution by most software engineers is to use SSH keys. These keys are encrypted and locally protected by a password. This adds security but also ease of use. Make sure that SSH authentication works for your case. By default SSH will operate on port 22. Due to the \"port 22 vulnerability\", this port is often closed. When using SSH authentication, make sure that you can find a suitable port. When using SSH authentication you will generate your public and private keys using the ssh-keygen tool. You public key is then uploaded to the server, where it can be used for authentication.","title":"Authentication in Git"},{"location":"Git/authentication/#authentication-in-git","text":"When connecting to a repository, you will need to identify yourself and authenticate with the server. There are two ways in which the authentication can be done: Using passwordsm or using SSH keys.","title":"Authentication in Git"},{"location":"Git/authentication/#password-based-authentication","text":"The password based authentication is fairly common since it is easy to set up. In this case you connect to the repository using your username and password. This method is not secure and therefore often not the first choice. The username and passwords are not encrypted and simply stored in plain text format. If you use this method make sure you use it on a personal computer with limited access. On Windows machines the default credential manager ( wincred ) is used to store these secrets. If this is not the case, you can set this up using the command below. git config --global credential.helper wincred On linux (WSL to be specific) there is no such default setting. When working VSCode will prompt you for credentials every 15 minutes (expiry of password). There are two options to resolve this. One method is to use the internal credential store. This can be set up using. git config --global credential.helper store An alternative is to increase the cache timeout, keeping the secrets in cache longer. The below example increases the timeout to two weeks in stead of the default 15 minutes. git config --global credential.helper 'cache --timeout==1209600' If you are using MacOS, the default credentials manager is osxkeychain. In this case you do the necessary setup using the command below. On MacOS git config --global credential.helper osxkeychain","title":"Password based authentication"},{"location":"Git/authentication/#ssh-key-based-authentication","text":"The preferred solution by most software engineers is to use SSH keys. These keys are encrypted and locally protected by a password. This adds security but also ease of use. Make sure that SSH authentication works for your case. By default SSH will operate on port 22. Due to the \"port 22 vulnerability\", this port is often closed. When using SSH authentication, make sure that you can find a suitable port. When using SSH authentication you will generate your public and private keys using the ssh-keygen tool. You public key is then uploaded to the server, where it can be used for authentication.","title":"SSH Key based authentication"},{"location":"Git/setting-up-git/","text":"Setting up Git Setup of global variables When running Git for the first time you will need to configure your username and password. For this you can use the code snippet below. $ git config --global user.name \"John Doe\" $ git config --global user.email johndoe@example.com This is the minimally required metadata for a successful setup in Git.","title":"Setting up Git"},{"location":"Git/setting-up-git/#setting-up-git","text":"","title":"Setting up Git"},{"location":"Git/setting-up-git/#setup-of-global-variables","text":"When running Git for the first time you will need to configure your username and password. For this you can use the code snippet below. $ git config --global user.name \"John Doe\" $ git config --global user.email johndoe@example.com This is the minimally required metadata for a successful setup in Git.","title":"Setup of global variables"},{"location":"Linux/","text":"Linux A popular open source OS that has many different flavors.","title":"Linux"},{"location":"Linux/#linux","text":"A popular open source OS that has many different flavors.","title":"Linux"},{"location":"Linux/windows-subsystem-for-linux/","text":"Windows Subsystem for Linux (WSL)","title":"Windows Subsystem for Linux (WSL)"},{"location":"Linux/windows-subsystem-for-linux/#windows-subsystem-for-linux-wsl","text":"","title":"Windows Subsystem for Linux (WSL)"},{"location":"Python/","text":"Python","title":"Python"},{"location":"Python/#python","text":"","title":"Python"},{"location":"Python/cli-tooling/","text":"Command Line Interface Writing a Command Line Interface (CLI) tool. Getting the arguments Easiest way is to use the argparse package. For example we could write a CLI that will greet the user who calls it (see code snippet below). The name is provided as the argument -n or --name . greeting.py import argparse def say_hello ( name ): message = f \"Hello { name } !\" print ( message ) return message def main (): # Get the arguments from the command line parser = argparse . ArgumentsParser () group = parser . add_mutually_exclusive_group () group . add_argument ( '-n' , '--name' , default = 'python' ) args = parser . parse_args () # Fetch the name and say hello name = args . name message = say_hello ( name ) return True if __name__ == '__main__' : main () You might also encounter sys.argv in solutions to this problem. The sys.argv variable contains all the arguments that are typed by the user when the script is called in a list. This is less clean and requires more code to neatly manage this. For example, if we run the call in the next section the sys.argv variable would be equal to: [ '--name' , 'Nick' ] . In this case you would need to write your own parsers to do error checking and to catch the correct synonyms. Calling the python CLI tool To call a script that works with CLI, you can simply use python. python greeting.py --name Nick Using the console to call python scripts is exteremely powerful as this allows you to create automation tools using python.","title":"Command Line Interface"},{"location":"Python/cli-tooling/#command-line-interface","text":"Writing a Command Line Interface (CLI) tool.","title":"Command Line Interface"},{"location":"Python/cli-tooling/#getting-the-arguments","text":"Easiest way is to use the argparse package. For example we could write a CLI that will greet the user who calls it (see code snippet below). The name is provided as the argument -n or --name . greeting.py import argparse def say_hello ( name ): message = f \"Hello { name } !\" print ( message ) return message def main (): # Get the arguments from the command line parser = argparse . ArgumentsParser () group = parser . add_mutually_exclusive_group () group . add_argument ( '-n' , '--name' , default = 'python' ) args = parser . parse_args () # Fetch the name and say hello name = args . name message = say_hello ( name ) return True if __name__ == '__main__' : main () You might also encounter sys.argv in solutions to this problem. The sys.argv variable contains all the arguments that are typed by the user when the script is called in a list. This is less clean and requires more code to neatly manage this. For example, if we run the call in the next section the sys.argv variable would be equal to: [ '--name' , 'Nick' ] . In this case you would need to write your own parsers to do error checking and to catch the correct synonyms.","title":"Getting the arguments"},{"location":"Python/cli-tooling/#calling-the-python-cli-tool","text":"To call a script that works with CLI, you can simply use python. python greeting.py --name Nick Using the console to call python scripts is exteremely powerful as this allows you to create automation tools using python.","title":"Calling the python CLI tool"},{"location":"Python/config-management/","text":"Config management The general idea of config management is to seperate the \"logic\" (or \"application layer\") from the \"specifics\" (or \"information layer\"). The logic is then handled by the (immutable) code base, where the \"specifics\" are handled by the external configurations. Typically you will have two parts in your software to handle the \"specifics\": A config file: This file contains all the configurations for the application that you are running. A config module: This module will search for config files and load these. The config file Typically the config file is a JSON file that is stored in a fixed location 1 . Within the JSON file, the arguments are then provided in the form of name-value pairs. config.json { \"name\" : \"Nick\" } Note that, in case of missing config files, a default config setting should be provided. These default config settings can be provided somewhere internally in the package itself. default_config.json { \"name\" : \"Python\" } Always try to provide default locations to place the config files. Typically this file is placed at the root level of the application. Although you could also provide a custom location. The key is consistency and good conventions. This will also allow your application to become scalable. Note that, as your application grows, you might find the need to introduce more than one config file, where each config file focusses on a separate part of the software. The config.py module In the module, we can define our config using a 'get_path' and 'get_config' function. In this simple case you can then assign your configs to a dictionary that allows you to fetch the different config values. The problem with this approach is that a python dictionary is not immutable 2 . A solution to this 'immutable' problem is to use the class 'property'. With the 'property' decorator, the class method is converted to a 'getter' function, where the name of the method becomes the protected class attribute. You can also add 'setter' and 'deleter' methods to that property in order to set or delete the value in a defined and secure way 3 . Next to packaging the config in a class you can additionally: Fetch the attributes via a private function (e.g. _set_name(self) ) in the function below. When doing this, the function set_name will be called every time the attribute is read. This is useful for long-running applications where the configs can change during runtime. By re-reading you will always have the most recent version of your config files. Config files are useful for values that should not change that frequently. If you have runtime variables that tend to change often, these can be added as environment variabels. In this case you will read the value via the os.environ.get('name') . config.py import os import json _user_config_path = \"<path where user specific settings can be provided>\" _global_config_path = \"<path where global settings can be provided>\" _default_config_path = \"<path to default config file if all is blank>\" def get_config_path () -> str : for path in [ _user_config_path , _global_config_path ]: if os . path . exists ( path ): return path return default_config_path def get_config () -> dict : with open ( self . config_path , 'rt' ) as config : config_dict = json . load ( config ) return config_dict class _Config (): def __init__ ( self ): \"\"\"This method is blank as everything is fetched from config. \"\"\" pass @property def name ( self ) -> str : return self . _set_name () def _set_name ( self ) -> str : return self . _config . get ( 'name) # At the end we instantiate the class in the CONFIG variable. CONFIG = _Config () To use the config file, you simply do from .config import CONFIG . Every attribute can then be fetched using the CONFIG.<attribute> . In the above case we have for example CONFIG.name . Additional notes There appear to be other packages that can be useful for this as well: Hydra pydantic dynaconf Still to be seen/tested. As an alternative, you could also work any type of format. Important is to keep the \"ease of use\" and portability in mind. For example, XML files might be not so easy to handle by the end user (or easy to read by the developer). Popular file formats are: JSON, YAML and INI. Note that sometimes also SQL databases might get used depending on the use-case. \u21a9 The basic assumption of python is on the other hand that \"everyone is an adult\" so we should not impose some \"child locks\" in the software. Of course, accidents can happen so it's always a good practice to keep some safeguards never the less. \u21a9 In our case no setters and deleters were added since we want to keep the config settings immutable . They should only be changed via the config file. \u21a9","title":"Config management"},{"location":"Python/config-management/#config-management","text":"The general idea of config management is to seperate the \"logic\" (or \"application layer\") from the \"specifics\" (or \"information layer\"). The logic is then handled by the (immutable) code base, where the \"specifics\" are handled by the external configurations. Typically you will have two parts in your software to handle the \"specifics\": A config file: This file contains all the configurations for the application that you are running. A config module: This module will search for config files and load these.","title":"Config management"},{"location":"Python/config-management/#the-config-file","text":"Typically the config file is a JSON file that is stored in a fixed location 1 . Within the JSON file, the arguments are then provided in the form of name-value pairs. config.json { \"name\" : \"Nick\" } Note that, in case of missing config files, a default config setting should be provided. These default config settings can be provided somewhere internally in the package itself. default_config.json { \"name\" : \"Python\" } Always try to provide default locations to place the config files. Typically this file is placed at the root level of the application. Although you could also provide a custom location. The key is consistency and good conventions. This will also allow your application to become scalable. Note that, as your application grows, you might find the need to introduce more than one config file, where each config file focusses on a separate part of the software.","title":"The config file"},{"location":"Python/config-management/#the-configpy-module","text":"In the module, we can define our config using a 'get_path' and 'get_config' function. In this simple case you can then assign your configs to a dictionary that allows you to fetch the different config values. The problem with this approach is that a python dictionary is not immutable 2 . A solution to this 'immutable' problem is to use the class 'property'. With the 'property' decorator, the class method is converted to a 'getter' function, where the name of the method becomes the protected class attribute. You can also add 'setter' and 'deleter' methods to that property in order to set or delete the value in a defined and secure way 3 . Next to packaging the config in a class you can additionally: Fetch the attributes via a private function (e.g. _set_name(self) ) in the function below. When doing this, the function set_name will be called every time the attribute is read. This is useful for long-running applications where the configs can change during runtime. By re-reading you will always have the most recent version of your config files. Config files are useful for values that should not change that frequently. If you have runtime variables that tend to change often, these can be added as environment variabels. In this case you will read the value via the os.environ.get('name') . config.py import os import json _user_config_path = \"<path where user specific settings can be provided>\" _global_config_path = \"<path where global settings can be provided>\" _default_config_path = \"<path to default config file if all is blank>\" def get_config_path () -> str : for path in [ _user_config_path , _global_config_path ]: if os . path . exists ( path ): return path return default_config_path def get_config () -> dict : with open ( self . config_path , 'rt' ) as config : config_dict = json . load ( config ) return config_dict class _Config (): def __init__ ( self ): \"\"\"This method is blank as everything is fetched from config. \"\"\" pass @property def name ( self ) -> str : return self . _set_name () def _set_name ( self ) -> str : return self . _config . get ( 'name) # At the end we instantiate the class in the CONFIG variable. CONFIG = _Config () To use the config file, you simply do from .config import CONFIG . Every attribute can then be fetched using the CONFIG.<attribute> . In the above case we have for example CONFIG.name .","title":"The config.py module"},{"location":"Python/config-management/#additional-notes","text":"There appear to be other packages that can be useful for this as well: Hydra pydantic dynaconf Still to be seen/tested. As an alternative, you could also work any type of format. Important is to keep the \"ease of use\" and portability in mind. For example, XML files might be not so easy to handle by the end user (or easy to read by the developer). Popular file formats are: JSON, YAML and INI. Note that sometimes also SQL databases might get used depending on the use-case. \u21a9 The basic assumption of python is on the other hand that \"everyone is an adult\" so we should not impose some \"child locks\" in the software. Of course, accidents can happen so it's always a good practice to keep some safeguards never the less. \u21a9 In our case no setters and deleters were added since we want to keep the config settings immutable . They should only be changed via the config file. \u21a9","title":"Additional notes"},{"location":"Python/documentation/","text":"Documentation mkdocs With mkdocs you can automatically generate a static website from markdown files. Start a new project. mkdocs new my-project Generate documentation. mkdocs serve Build the documentation. mkdocs build Sphinx With Sphinx you can automatically generate documentation for python projects.","title":"Documentation"},{"location":"Python/documentation/#documentation","text":"","title":"Documentation"},{"location":"Python/documentation/#mkdocs","text":"With mkdocs you can automatically generate a static website from markdown files. Start a new project. mkdocs new my-project Generate documentation. mkdocs serve Build the documentation. mkdocs build","title":"mkdocs"},{"location":"Python/documentation/#sphinx","text":"With Sphinx you can automatically generate documentation for python projects.","title":"Sphinx"},{"location":"Python/environments/","text":"Environments For python we have serval environments defined based on a set of YAML files. Setting up a virtual environment Working with virtual (isolated) environments is strongly encouraged as this will increase the portability of your software project. (add footnote: Installing all packages in your \"base\" installation is also considered bad practice. The reason for this is that packages can interfere with each other, leading to unexpected bugs.) There are many options to create a virtual environment. The two most popular ways are: Using conda: This is the easiest way to start. In this case there is a visual user interface and integration with your (Power)Shell. Also conda can manage other packages for you as well. Using pipenv: Here you manage the environments (location) yourself. This gives you more control. Also pipenv appears to be more suited to work with packages. An additional feature in pipenv is to label packages as --dev packages (avoiding that these get pushed to production). Note If you want to go next level, you can deploy your code as a containerized app. An example is to use docker to do this. Second note: Poetry is also a strongly upcoming package. The big advantage of working with the pyvenv virtual environments is that these are usually liteweight, allowing for fast prototyping. Using the pip list --format=freeze > requirements.txt command you can easily generate the requirements.txt file for further deployment. The conda virtual envrionments are typically used personally to set up full-fletched environments that contain too many packages. This is powerful to do a quick analysis or proof of value (PoV), or to keep a personal catalog of interesting packages. For a Proof of Concept (PoC) and beyond it is more beneficial to work with personally-managed pipvenv virtual environments. Note that .venv folders are typically part of the gitignore, forcing you to work with a requirements.txt file. Setting up an environment Installing based on a file is easy: Show both the conda and pip command using the \"toggle\". Very important is to fix the versions of your packages. pip conda python - m venv < location to your venv > conda create -- name < environment name > A virtual environment belongs to a project. Typically the virtual environment is installed in the directory './.venv' (when installed via pip). This way the virtual environment is specific for your project. Note that the virtual environment typically is not named. The preceding dot '.' in the location name makes it clear that the virtual environment is a special file. Normally a requirements.txt file is provided next tot he .venv folder so that the .venv can always be rebuilt from scratch. For conda, the virtual environments are not project specific. In the case of conda, the environments are installed in a central location. The different environments Place a reference to the different files. Notes (to do): Give the YAML file (conda flavor) Give the .txt file (pip flavor) Package versions should always be specified. In the provided examples, the package versions were not always specified. Note that this can lead to inconsistencies and unknown bugs when re-installing the environments. For a local poc/development setting this setup can be okay as you will first try to build the solution. When going into testing/production, the versions should be specified always in order to avoid unknown errors. pip conda For pip, the packages still need to be translated to the requirements.txt format. For native python and the pip (package installer for python) the environments are provided using txt files. To install an environment with pip using a requirements file, the command pip install -r <filename> can be used. Note that this command can also be used to update packages in your environment. To do this, you simply change the version in your requirements file and re-run the command, pip will detect the changes and update accordingly. For conda, the environments are defined using YAML files. To install en environment using conda, use the command conda env create -f <filename> . A full list of conda commands can be found in the conda cheatsheet . Automation Azure Functions Azure Machine Learning Big Data Computer Vision Data Analysis Deep Learning Documentation Machine Learning Network Analysis Operations Research Statistical Simulation","title":"Environments"},{"location":"Python/environments/#environments","text":"For python we have serval environments defined based on a set of YAML files.","title":"Environments"},{"location":"Python/environments/#setting-up-a-virtual-environment","text":"Working with virtual (isolated) environments is strongly encouraged as this will increase the portability of your software project. (add footnote: Installing all packages in your \"base\" installation is also considered bad practice. The reason for this is that packages can interfere with each other, leading to unexpected bugs.) There are many options to create a virtual environment. The two most popular ways are: Using conda: This is the easiest way to start. In this case there is a visual user interface and integration with your (Power)Shell. Also conda can manage other packages for you as well. Using pipenv: Here you manage the environments (location) yourself. This gives you more control. Also pipenv appears to be more suited to work with packages. An additional feature in pipenv is to label packages as --dev packages (avoiding that these get pushed to production). Note If you want to go next level, you can deploy your code as a containerized app. An example is to use docker to do this. Second note: Poetry is also a strongly upcoming package. The big advantage of working with the pyvenv virtual environments is that these are usually liteweight, allowing for fast prototyping. Using the pip list --format=freeze > requirements.txt command you can easily generate the requirements.txt file for further deployment. The conda virtual envrionments are typically used personally to set up full-fletched environments that contain too many packages. This is powerful to do a quick analysis or proof of value (PoV), or to keep a personal catalog of interesting packages. For a Proof of Concept (PoC) and beyond it is more beneficial to work with personally-managed pipvenv virtual environments. Note that .venv folders are typically part of the gitignore, forcing you to work with a requirements.txt file.","title":"Setting up a virtual environment"},{"location":"Python/environments/#setting-up-an-environment","text":"Installing based on a file is easy: Show both the conda and pip command using the \"toggle\". Very important is to fix the versions of your packages. pip conda python - m venv < location to your venv > conda create -- name < environment name > A virtual environment belongs to a project. Typically the virtual environment is installed in the directory './.venv' (when installed via pip). This way the virtual environment is specific for your project. Note that the virtual environment typically is not named. The preceding dot '.' in the location name makes it clear that the virtual environment is a special file. Normally a requirements.txt file is provided next tot he .venv folder so that the .venv can always be rebuilt from scratch. For conda, the virtual environments are not project specific. In the case of conda, the environments are installed in a central location.","title":"Setting up an environment"},{"location":"Python/environments/#the-different-environments","text":"Place a reference to the different files. Notes (to do): Give the YAML file (conda flavor) Give the .txt file (pip flavor) Package versions should always be specified. In the provided examples, the package versions were not always specified. Note that this can lead to inconsistencies and unknown bugs when re-installing the environments. For a local poc/development setting this setup can be okay as you will first try to build the solution. When going into testing/production, the versions should be specified always in order to avoid unknown errors. pip conda For pip, the packages still need to be translated to the requirements.txt format. For native python and the pip (package installer for python) the environments are provided using txt files. To install an environment with pip using a requirements file, the command pip install -r <filename> can be used. Note that this command can also be used to update packages in your environment. To do this, you simply change the version in your requirements file and re-run the command, pip will detect the changes and update accordingly. For conda, the environments are defined using YAML files. To install en environment using conda, use the command conda env create -f <filename> . A full list of conda commands can be found in the conda cheatsheet . Automation Azure Functions Azure Machine Learning Big Data Computer Vision Data Analysis Deep Learning Documentation Machine Learning Network Analysis Operations Research Statistical Simulation","title":"The different environments"},{"location":"Python/makefiles/","text":"Makefiles A good makefile structure for python.","title":"Makefiles"},{"location":"Python/makefiles/#makefiles","text":"A good makefile structure for python.","title":"Makefiles"},{"location":"Python/packages-in-python/","text":"Packages in python Importing packages in python The four most popular ways to import libraries and functions into python are: import library import library_with_very_long_name as lwvln from library import function1, function2 from library import * Never do a 'blanket import' (option 4)! The problem with the blanket import is that you are importing all functions of a given package into your global namespace. This can become problematic if your imported library is overwriting a function that already exists in your namespace as this can break your existing code. To make things even worse, when the package changes versions (update/downgrade/bugfix), new functions can unknowingly be introduced which might in their turn again overwrite existing functions in your namespace. From the remaining list of options (first three choices), the style of import strongly depends on how you intend to use the functions in your module. A good guideline for choosing the appropriate import statement is the \"Zen of Python\" (PEP 20). To print out the PEP 20 in a python session, just run the command import this . The two useful guidelines to follow when it comes to importing packages are: \"Explicit is better than implicit.\": Exactly knowing which package you are using will allow you to trace where an error comes from. \"Readability counts.\": Clarity triumphs over brevity. If the code is easier to read by adding a few characters, then by all means go ahead. General advise on importing packages Each import statement is ideally placed on a separate line. This will make the code more readable. An example is shown in the two code snippets below. bad_import.py import sys , os good_import.py import sys import os Preferred style of import statement My personal preference for importing packages is according to the list above: First try import library : If the library name is short, this should be the default mode. Note also that most packages are only used once or twice within the same script, so typing out the full name should not create much clutter. For frequently used library with a long name, the import library_with_very_long_name as lwvln is a good option. For packages that are used often, the alias will be easily recognized during usage. Note that this should only be used for frequently used packages. For infrequently used packages, the alias can be unclear. The direct function import from library import function1, function2 has the potential to overwrite existing functions in the namespace, but this effect is fairly limited. This style of import is typically also not encouraged but could be used if needed for functions that are very deep in a function hierarchy, or when the chances of function name clashes are very small (in the case of specific naming). For deep hierarchies such as library.module.submodule , you could also import the submodule directly using from library.module import submodule , this can reduce the amount of typing, while keeping the code fairly explicit (although the alias might then also help). A popular example where this style is used is the datetime package, where you will often find from datetime import datetime . All of the above options are known as \"explicit imports\", as you are explicitly stating what you want to import into your code. By using explicit imports, refactoring code is easier! By explicitly mentioning the function and library names, it is easier to keep track of their usage in your code. This can come in handy when refactoring your code to avoid unnecessary import statements. Be careful with lazy imports Import statements add some overhead time to running your script. Sometimes, to speed up the initial start up time of the script, imports are moved inside of functions that use the function that will use the code. This way, the packages will not be imported on startup, but only when the function is first executed. An example of a lazy import is shown below. function_with_lazy_import.py def greeting_lower ( text ): import string return string . lower ( text ) Lazy imports can hurt your performance! Even though python will never import the same package twice, it will still perform a \"check\" when an import statement is ran. If you have a function with explicit import that is ran multiple times, this means the check is also ran multiple times, costing you time. IF you use lazy imports, always make sure that you use it for packages that are specific to that function. Moreover, also make sure that the function is not used very frequently. If both cases are met, the function is a good candidate for a lazy import.","title":"Packages in python"},{"location":"Python/packages-in-python/#packages-in-python","text":"","title":"Packages in python"},{"location":"Python/packages-in-python/#importing-packages-in-python","text":"The four most popular ways to import libraries and functions into python are: import library import library_with_very_long_name as lwvln from library import function1, function2 from library import * Never do a 'blanket import' (option 4)! The problem with the blanket import is that you are importing all functions of a given package into your global namespace. This can become problematic if your imported library is overwriting a function that already exists in your namespace as this can break your existing code. To make things even worse, when the package changes versions (update/downgrade/bugfix), new functions can unknowingly be introduced which might in their turn again overwrite existing functions in your namespace. From the remaining list of options (first three choices), the style of import strongly depends on how you intend to use the functions in your module. A good guideline for choosing the appropriate import statement is the \"Zen of Python\" (PEP 20). To print out the PEP 20 in a python session, just run the command import this . The two useful guidelines to follow when it comes to importing packages are: \"Explicit is better than implicit.\": Exactly knowing which package you are using will allow you to trace where an error comes from. \"Readability counts.\": Clarity triumphs over brevity. If the code is easier to read by adding a few characters, then by all means go ahead.","title":"Importing packages in python"},{"location":"Python/packages-in-python/#general-advise-on-importing-packages","text":"Each import statement is ideally placed on a separate line. This will make the code more readable. An example is shown in the two code snippets below. bad_import.py import sys , os good_import.py import sys import os","title":"General advise on importing packages"},{"location":"Python/packages-in-python/#preferred-style-of-import-statement","text":"My personal preference for importing packages is according to the list above: First try import library : If the library name is short, this should be the default mode. Note also that most packages are only used once or twice within the same script, so typing out the full name should not create much clutter. For frequently used library with a long name, the import library_with_very_long_name as lwvln is a good option. For packages that are used often, the alias will be easily recognized during usage. Note that this should only be used for frequently used packages. For infrequently used packages, the alias can be unclear. The direct function import from library import function1, function2 has the potential to overwrite existing functions in the namespace, but this effect is fairly limited. This style of import is typically also not encouraged but could be used if needed for functions that are very deep in a function hierarchy, or when the chances of function name clashes are very small (in the case of specific naming). For deep hierarchies such as library.module.submodule , you could also import the submodule directly using from library.module import submodule , this can reduce the amount of typing, while keeping the code fairly explicit (although the alias might then also help). A popular example where this style is used is the datetime package, where you will often find from datetime import datetime . All of the above options are known as \"explicit imports\", as you are explicitly stating what you want to import into your code. By using explicit imports, refactoring code is easier! By explicitly mentioning the function and library names, it is easier to keep track of their usage in your code. This can come in handy when refactoring your code to avoid unnecessary import statements.","title":"Preferred style of import statement"},{"location":"Python/packages-in-python/#be-careful-with-lazy-imports","text":"Import statements add some overhead time to running your script. Sometimes, to speed up the initial start up time of the script, imports are moved inside of functions that use the function that will use the code. This way, the packages will not be imported on startup, but only when the function is first executed. An example of a lazy import is shown below. function_with_lazy_import.py def greeting_lower ( text ): import string return string . lower ( text ) Lazy imports can hurt your performance! Even though python will never import the same package twice, it will still perform a \"check\" when an import statement is ran. If you have a function with explicit import that is ran multiple times, this means the check is also ran multiple times, costing you time. IF you use lazy imports, always make sure that you use it for packages that are specific to that function. Moreover, also make sure that the function is not used very frequently. If both cases are met, the function is a good candidate for a lazy import.","title":"Be careful with lazy imports"},{"location":"Python/packaging-projects/","text":"Packaging projects Creating a package setup.py and __init__.py files to make the package installable. An example on a typical setup script is shown below, where you are installing a package in the current directory. The __init__.py file is just a listing of default functions that are imported when importing the package, or which code to run on import. If the init file is left empty, python will still recognize it as a package but perform no further actions on import. setup.py from glob import glob from os.path import basename , splitext from setuptools import find_packages , setup setup ( name = 'my_package' , version = '0.1' , packages = find_packages ( where = 'src' ), package_dir = { '' : 'src' }, py_modules = [ splitext ( basename ( path ))[ 0 ] for path in glob ( 'src/*.py' )], ) Installing a local package If you move to the folder that contains the package, you can run pip install . , this command will tell pip to install the packages in your current directory. The package is then added to the site-packages . Another command that is useful to know is pip install --editable . . By adding the --editable flag, we are telling pip to read the code in its original location in stead of copying it to the python site-packages . In this case, pip will create a reference to the code. Each time you run the import, you are reading the latest version of your source files. The advantage of this is that you can iteratively build up your python package as you go. This procedure will make your code reachable from any directory. This is a big simplification. If you do not do this, then you will need to add your file containing the code to the system path in order to be able to import the python package. For example, if your code is placed in a folder '../src', you can add the path using the code snippet below. Once the path is imported, you can read the python files as if it were packages. import sys sys . path . append ( '../src' ) Packaging a package Build a wheel file. Reloading imported packages in jupyter In jupyter notebooks, python imports are chached. This means that once you have imported something that it will not be reloaded (net even when re-running the import statement). If you are actively working on source files, the changes of the package need to be incorporated on the fly. So the caching behavior is then unwanted. Using IPython and notebook magic, it is possible to enable the auto-reloading feature using the code snippet below. % load_ext autoreload % autoreload 2","title":"Packaging projects"},{"location":"Python/packaging-projects/#packaging-projects","text":"","title":"Packaging projects"},{"location":"Python/packaging-projects/#creating-a-package","text":"setup.py and __init__.py files to make the package installable. An example on a typical setup script is shown below, where you are installing a package in the current directory. The __init__.py file is just a listing of default functions that are imported when importing the package, or which code to run on import. If the init file is left empty, python will still recognize it as a package but perform no further actions on import. setup.py from glob import glob from os.path import basename , splitext from setuptools import find_packages , setup setup ( name = 'my_package' , version = '0.1' , packages = find_packages ( where = 'src' ), package_dir = { '' : 'src' }, py_modules = [ splitext ( basename ( path ))[ 0 ] for path in glob ( 'src/*.py' )], )","title":"Creating a package"},{"location":"Python/packaging-projects/#installing-a-local-package","text":"If you move to the folder that contains the package, you can run pip install . , this command will tell pip to install the packages in your current directory. The package is then added to the site-packages . Another command that is useful to know is pip install --editable . . By adding the --editable flag, we are telling pip to read the code in its original location in stead of copying it to the python site-packages . In this case, pip will create a reference to the code. Each time you run the import, you are reading the latest version of your source files. The advantage of this is that you can iteratively build up your python package as you go. This procedure will make your code reachable from any directory. This is a big simplification. If you do not do this, then you will need to add your file containing the code to the system path in order to be able to import the python package. For example, if your code is placed in a folder '../src', you can add the path using the code snippet below. Once the path is imported, you can read the python files as if it were packages. import sys sys . path . append ( '../src' )","title":"Installing a local package"},{"location":"Python/packaging-projects/#packaging-a-package","text":"Build a wheel file.","title":"Packaging a package"},{"location":"Python/packaging-projects/#reloading-imported-packages-in-jupyter","text":"In jupyter notebooks, python imports are chached. This means that once you have imported something that it will not be reloaded (net even when re-running the import statement). If you are actively working on source files, the changes of the package need to be incorporated on the fly. So the caching behavior is then unwanted. Using IPython and notebook magic, it is possible to enable the auto-reloading feature using the code snippet below. % load_ext autoreload % autoreload 2","title":"Reloading imported packages in jupyter"},{"location":"Python/quality-assurance/","text":"Quality Assurance Here we follow the hierarchy of the theory section on QA. Automated testing tools Linting Calling a linter is easy as these are just python modules: python3 -m pip install flake8 will for example install the flake8 module. The linter can then be used by calling TODO: Also add part on pylint and make a toggle to choose. Unit testing pytest and pytest-cov are two packages to use for this TODO: Add code examples in python. Manual testing tools Load testing Popular load-testing packages for python are: molotov locust.io loader.io Visual testing TODO: Add packages to test graphs.","title":"Quality Assurance"},{"location":"Python/quality-assurance/#quality-assurance","text":"Here we follow the hierarchy of the theory section on QA.","title":"Quality Assurance"},{"location":"Python/quality-assurance/#automated-testing-tools","text":"","title":"Automated testing tools"},{"location":"Python/quality-assurance/#linting","text":"Calling a linter is easy as these are just python modules: python3 -m pip install flake8 will for example install the flake8 module. The linter can then be used by calling TODO: Also add part on pylint and make a toggle to choose.","title":"Linting"},{"location":"Python/quality-assurance/#unit-testing","text":"pytest and pytest-cov are two packages to use for this TODO: Add code examples in python.","title":"Unit testing"},{"location":"Python/quality-assurance/#manual-testing-tools","text":"","title":"Manual testing tools"},{"location":"Python/quality-assurance/#load-testing","text":"Popular load-testing packages for python are: molotov locust.io loader.io","title":"Load testing"},{"location":"Python/quality-assurance/#visual-testing","text":"TODO: Add packages to test graphs.","title":"Visual testing"},{"location":"Python/saving-ml-models/","text":"Saving ML models When a model is done training, you want to save it in order to quickly use the model without retraining every time. Pickled files When pickling a file (using joblib), python will export any python object into a stream of bytes. This method is extremely fast. The downside is that models are often not interchangeable between users and projects, and worse python versions. If you are working solo or on a simple proof of concept or with temporary files, this format has its merits. But note that at some point in time pickled files are doomed to break down. Using JSON In this case you will manually export your model by defining your parameters one by one and exporting these to Using PMML The Predictive Model Markup Language (PMML) format is a common XML format used to save models in data mining and statistical modeling. To save models in the PMML format, you use the pypmml package in python. This allows you to package entire pipelines in a few lines of code. Using Tensorflow/Keras In this case the model is saved to an \"h5\" file. This is a file saved in the Hierarchical Data Fromat (HDF). This datatype contains multidimensional arrays, which makes it very suitable for storing large data and is therefore commonly used in nearly al fields of science. Due to its format it is also very suitable for storing neural networks. This way of saving models has been build natively into tensorflow and Keras and will therefore be available via the save() method on any model, e.g.: model.save('path/to/model.h5) . To load a model, you import Keras from tensorflow and use keras.models.load_model('path/to/model.h5) . Using ONNX The newest format built to represent machine learning models.","title":"Saving ML models"},{"location":"Python/saving-ml-models/#saving-ml-models","text":"When a model is done training, you want to save it in order to quickly use the model without retraining every time.","title":"Saving ML models"},{"location":"Python/saving-ml-models/#pickled-files","text":"When pickling a file (using joblib), python will export any python object into a stream of bytes. This method is extremely fast. The downside is that models are often not interchangeable between users and projects, and worse python versions. If you are working solo or on a simple proof of concept or with temporary files, this format has its merits. But note that at some point in time pickled files are doomed to break down.","title":"Pickled files"},{"location":"Python/saving-ml-models/#using-json","text":"In this case you will manually export your model by defining your parameters one by one and exporting these to","title":"Using JSON"},{"location":"Python/saving-ml-models/#using-pmml","text":"The Predictive Model Markup Language (PMML) format is a common XML format used to save models in data mining and statistical modeling. To save models in the PMML format, you use the pypmml package in python. This allows you to package entire pipelines in a few lines of code.","title":"Using PMML"},{"location":"Python/saving-ml-models/#using-tensorflowkeras","text":"In this case the model is saved to an \"h5\" file. This is a file saved in the Hierarchical Data Fromat (HDF). This datatype contains multidimensional arrays, which makes it very suitable for storing large data and is therefore commonly used in nearly al fields of science. Due to its format it is also very suitable for storing neural networks. This way of saving models has been build natively into tensorflow and Keras and will therefore be available via the save() method on any model, e.g.: model.save('path/to/model.h5) . To load a model, you import Keras from tensorflow and use keras.models.load_model('path/to/model.h5) .","title":"Using Tensorflow/Keras"},{"location":"Python/saving-ml-models/#using-onnx","text":"The newest format built to represent machine learning models.","title":"Using ONNX"},{"location":"Python/type-hinting/","text":"Type hinting in python Supported since python version 3.5. With type hinting, you give 'hints' on the datatype in the code itself. By doing this, you provide context to the code, making it easier to check statically (by a linter). By using type hinting, you can add an additional layer to your stack of debugging tools Type hinting only operates during static type checking. Note that type hinting is only used during static type checking. At runtime, the code will evaluate. hello_name.py def hello_name ( name : str = \"python\" ) -> str : message = \"Hello {name} !\" return message","title":"Type hinting in python"},{"location":"Python/type-hinting/#type-hinting-in-python","text":"Supported since python version 3.5. With type hinting, you give 'hints' on the datatype in the code itself. By doing this, you provide context to the code, making it easier to check statically (by a linter). By using type hinting, you can add an additional layer to your stack of debugging tools Type hinting only operates during static type checking. Note that type hinting is only used during static type checking. At runtime, the code will evaluate. hello_name.py def hello_name ( name : str = \"python\" ) -> str : message = \"Hello {name} !\" return message","title":"Type hinting in python"},{"location":"References/","text":"References","title":"References"},{"location":"References/#references","text":"","title":"References"},{"location":"References/data-science/","text":"Data Science A collection of resources concerning Data Science and ML engineering. Courses Books","title":"Data Science"},{"location":"References/data-science/#data-science","text":"A collection of resources concerning Data Science and ML engineering.","title":"Data Science"},{"location":"References/data-science/#courses","text":"","title":"Courses"},{"location":"References/data-science/#books","text":"","title":"Books"},{"location":"SQL/","text":"SQL","title":"SQL"},{"location":"SQL/#sql","text":"","title":"SQL"},{"location":"SQL/database-information/","text":"Get all tables and their sizes Read the different database tables to obtain table sizes. SELECT t . Name AS TableName , s . Name AS SchemaName , p . Rows AS RowCounts , SUM ( a . total_pages ) * 8 AS TotalSpaceKB , SUM ( a . used_pages ) * 8 AS UsedSpaceKB , ( SUM ( a . total_pages ) - SUM ( a . used_pages )) * 8 AS UnusedSpaceKB FROM sys . tables t INNER JOIN sys . indexes i ON t . object_id = i . object_id INNER JOIN sys . partitions p ON i . object_id = p . object_id AND i . index_id = p . index_id INNER JOIN sys . allocation_units a ON p . partition_id = a . container_id LEFT OUTER JOIN sys . schemas s ON t . schema_id = s . schema_id WHERE t . Name NOT LIKE 'dt%' AND t . is_ms_shipped = 0 AND i . object_id > 255 GROUP BY t . Name , s . Name , p . Rows ORDER BY t . Name ; GO","title":"Get all tables and their sizes"},{"location":"SQL/database-information/#get-all-tables-and-their-sizes","text":"Read the different database tables to obtain table sizes. SELECT t . Name AS TableName , s . Name AS SchemaName , p . Rows AS RowCounts , SUM ( a . total_pages ) * 8 AS TotalSpaceKB , SUM ( a . used_pages ) * 8 AS UsedSpaceKB , ( SUM ( a . total_pages ) - SUM ( a . used_pages )) * 8 AS UnusedSpaceKB FROM sys . tables t INNER JOIN sys . indexes i ON t . object_id = i . object_id INNER JOIN sys . partitions p ON i . object_id = p . object_id AND i . index_id = p . index_id INNER JOIN sys . allocation_units a ON p . partition_id = a . container_id LEFT OUTER JOIN sys . schemas s ON t . schema_id = s . schema_id WHERE t . Name NOT LIKE 'dt%' AND t . is_ms_shipped = 0 AND i . object_id > 255 GROUP BY t . Name , s . Name , p . Rows ORDER BY t . Name ; GO","title":"Get all tables and their sizes"},{"location":"SQL/sql-information-schema/","text":"SQL Information Schema The SQL schema 'INFORMATION_SCHEMA' contains valuable information on the metadata present in the SQL database. Get information on all columns in the database SELECT * FROM INFORMATION_SCHEMA . columns","title":"SQL Information Schema"},{"location":"SQL/sql-information-schema/#sql-information-schema","text":"The SQL schema 'INFORMATION_SCHEMA' contains valuable information on the metadata present in the SQL database.","title":"SQL Information Schema"},{"location":"SQL/sql-information-schema/#get-information-on-all-columns-in-the-database","text":"SELECT * FROM INFORMATION_SCHEMA . columns","title":"Get information on all columns in the database"},{"location":"SQL/user-management/","text":"User Management Creating a user with name and password (SQL authentication) Note that two types of 'logins' should be created: A SQL server level login (using CREATE LOGIN ): This will grant a user/entity access to the server (or SQL instance). A SQL database level login (using CREATE USER ): This will allow a user/entity to authorize itself into a SQL database. Note: One login can be associated with many users CREATE LOGIN < login_name > WITH PASSWORD =< password_as_string > ; CREATE USER < user_name > FROM LOGIN < login_name > Once the user is created, you can add different roles to that user. Note, the login_name can be flat text, the password should be explicitly formatted as a string. Creating an Azure Active Directory (AAD) user CREATE USER < aad_user_id > FROM EXTERNAL PROVIDER ; In this case the server-level authentication is provided by AAD RBAC: user should also be added to the database on Azure (via IAM). The AAD user ID is the e-mail address.","title":"User Management"},{"location":"SQL/user-management/#user-management","text":"","title":"User Management"},{"location":"SQL/user-management/#creating-a-user-with-name-and-password-sql-authentication","text":"Note that two types of 'logins' should be created: A SQL server level login (using CREATE LOGIN ): This will grant a user/entity access to the server (or SQL instance). A SQL database level login (using CREATE USER ): This will allow a user/entity to authorize itself into a SQL database. Note: One login can be associated with many users CREATE LOGIN < login_name > WITH PASSWORD =< password_as_string > ; CREATE USER < user_name > FROM LOGIN < login_name > Once the user is created, you can add different roles to that user. Note, the login_name can be flat text, the password should be explicitly formatted as a string.","title":"Creating a user with name and password (SQL authentication)"},{"location":"SQL/user-management/#creating-an-azure-active-directory-aad-user","text":"CREATE USER < aad_user_id > FROM EXTERNAL PROVIDER ; In this case the server-level authentication is provided by AAD RBAC: user should also be added to the database on Azure (via IAM). The AAD user ID is the e-mail address.","title":"Creating an Azure Active Directory (AAD) user"},{"location":"Theory/","text":"Theory","title":"Theory"},{"location":"Theory/#theory","text":"","title":"Theory"},{"location":"Theory/coding-principles/","text":"Coding Principles Below the most common coding principles are described. Stay DRY. DRY (Don't Repeat Yourself): Try to re-use your code as much as possible. To achieve this, you can package your code into packages/functions and modules. This is a contrast to copy-pasting the same block of code over and over again. Advantage: If your code is broken, you only need to fix this in one central place. Keep It Simple & Straightforward (KISS). 'Simplicity is a prerequisite for reliability' ~ Edsger Dijkstra. The idea is to keep your code as simple and straightforward as possible. To do this, you split up your code into as simple and small blocks as possible. Each block should have a simple function without any side-effects. All blocks can then be combined in a \"main\" script in order to generate a complex logical flow. Note that this is also part of the \"Zen of Python\" (PEP 20): \"Simple is better than complex\". Yes, in the end you will have more lines of code, but these lines will be easier to maintain! Avoid hard coding. Hard coding settings into your software is always dangerous. The reason is that changing code will always bring risk. Better is to work with external config files so that configs are separated from logic. Another danger of hard-coding is the fact that code becomes harder to read as you are missing context. A typical example of this missing of context is the use of \"magic numbers\". These magic numbers are common in scripts where ad-hoc testing are performed. A magic number is a number that suddenly appears without any context. An example is shown in the code block below, where the number '52' seems to appear out of nowhere. ```python \"fisher-yates-magic.py\" def shuffle_items(items): import random for i in range(52): j = i + random.randrange(52) items[j], items[i] = items[i], items[j] return items In the above algorithm, we wanted to shuffle a deck of cards (52 cards). If we define this in a variable, the code suddenly becomes easier to read as we now have context (see below). ```python \"fisher-yates-shuffle-deck.py\" def shuffle_card_deck(items): import random card_deck_size = 52 for i in range(card_deck_size): j = i + random.randrange(card_deck_size) items[j], items[i] = items[i], items[j] return items Limit your line length. 'Code is read more often than it is written. Code should always be written in a way that promotes readability.' ~ Guido Van Rossum. This principle is mainly with the developer in mind to ease the readability of the code. A common rule is to limit the line width to 78-80 characters. This rule stems from the past where a computer screen could only contain 78 characters on a single line. More recent versions of this principle say to limit your code at 100-120 characters. Apart from \"screen width\" it should be noted that the optimal line length for a text to be \"readable\" is somewhere in the range 50-60 (some sources go as high as 75). Research found that longer line lengths may lead to reading mistakes (jumping back to the wrong line), resulting in a slower reading rate. This rule is for example implemented in the typesetting language LaTeX. But note that this rule is also used in printed books. Sign your code. This one is mostly about \"ownership and responsibility\". You should always strive to write code that you are proud of. If you are willing to actually \"sign\" your code, then this is an indication that it meets your personal quality standards. This ownership and responsibility should also be clear and open for everyone. Design to test. When writing code, always keep in mind that your code should be submitted to testing. A simple example of this is: always making sure that your code has a return value. If you want your code to be maintainable, this is a very strong principle. By adding unit tests to your code base, you can strongly increase the trust in your existing code. Fix the problem, not the blame. I first encountered this quote in the book \"The Pragmatic Programmer\". Afterwards I found out that this is actually a well-known Japanese proverb. The idea is that you should focus on the \"process\" rather than the \"people\". If you start the \"blame game\" you will end op with a broken system and a broken culture, while not fixing the problem. If you focus on the process, you will focus on \"best practices\" and the technical cause of the problem. When focussing on the process, you can work on self improvement and implement a \"growth mindset\" culture where you focus on improving the overall way of working. Explicit is better than implicit (keep it SMART). A principle that originates from the \"Zen of Python\" (PEP 20). In coding this is very important if you are thinking in terms of self-documentation and maintainability. Code where every line is completely clear is much easier to read than obscure code full of \"magic numbers\". However, in communication I am also a fan of this. Making things \"explicit\" makes sure that everyone is aligned and aware of what they are able to expect. When making proposals, making things explicit will allow others to either agree or to make a counter-proposal. In solution design (or project designs in general), making things explicit will also enable a better alignment. By adding specific milestones to the process you specify WHAT to deliver and WHEN to do it. In communication if you want things to be specific, the best acronym to remember is SMART. This means that every specific thing is Specific, Measurable, Acceptable, Realistic and Time-bound: Specific: Is there a clearly outlined goal that needs to be achieved? Measurable: What is the definition of done (DoD)? What metrics/KPIs can you use to measure the success? Acceptable: Are the goals also in line with what the customer/management is expecting? Realistic: Is this goal realistic? Time-bound: WHEN will these results be achieved? A clear committement on a deadline. Never make estimates that are too detailed. This idea is also neatly explained in the book: \"The Pragmatic Programmer\". There they also neatly explain how to generate a good estimate and update it using code. The general idea is that your estimate always (implicitly) reflects the accuracy in your estimate. For example, if you estimate in hours, the client will expect it to be correct up to several hours. If you on the other hand estimate in days, the expected accuracy is on the level of days. The longer your estimation, the less granular you should be with your initial estimate. Duration Quote estimate in 1 - 5 days (half) days 5-15 days days 3-8 weeks weeks 8-30 weeks months 30+ weeks think hard before giving an estimate. Silence is golden: the 4 second rule. Four seconds is all it takes for silence to get awkward. This is actually a very powerful tool in communication. By staying silent you give the other person the opportunity to also provide input. Often when brainstorming we have our biases and implicit assumptions. By allowing the other stakeholders to take the lead, we can see the problem from their perspective. If silence does not work, you can always follow it up with a set of open questions to check how the other stakeholders feel about the solution. Cunningham's law is your friend. Cunningham's law states: \"The best way to get the right answer (on the internet) is not to ask a question; it's to post the wrong answer.\". The underlying psychology is that people like to demonstrate when they know the answer. So if someone is giving you a hard time to get the solution of something, just say: \"I see, so it's actually <substitute wrong/random answer> \". You will see that most people can not resist to correct you then! This psychological effect is also very powerful when designing a solution with a client/stakeholder, or when working on a project. A good way to to use this effect during the design phase, or a technical discussion, is by saying something like: \"Am I understanding you correctly that: <Insert_assumption_that_has_been_made> \". If you are correct,the client will confirm, if you are wrong, you will immediately know. Never check your email in the morning. This is actually the title of the book that explains this principle. The general idea is to increase your focus time/productivity. The email (but also text messages etc) are a continuous flow of information. Because of our built-in FOMO (Fear Of Missing Out) we will always have the tendency to check these messages. The problem with this is that your workflow gets disturbed, making you loose your focus. The best thing to do is to explicitly \"plan\" when to read your email. This allows you to focus outside of these time-slots. Moreover, by planning your \"email-time\", you can also immediately react to the mail with your full focus.","title":"Coding Principles"},{"location":"Theory/coding-principles/#coding-principles","text":"Below the most common coding principles are described. Stay DRY. DRY (Don't Repeat Yourself): Try to re-use your code as much as possible. To achieve this, you can package your code into packages/functions and modules. This is a contrast to copy-pasting the same block of code over and over again. Advantage: If your code is broken, you only need to fix this in one central place. Keep It Simple & Straightforward (KISS). 'Simplicity is a prerequisite for reliability' ~ Edsger Dijkstra. The idea is to keep your code as simple and straightforward as possible. To do this, you split up your code into as simple and small blocks as possible. Each block should have a simple function without any side-effects. All blocks can then be combined in a \"main\" script in order to generate a complex logical flow. Note that this is also part of the \"Zen of Python\" (PEP 20): \"Simple is better than complex\". Yes, in the end you will have more lines of code, but these lines will be easier to maintain! Avoid hard coding. Hard coding settings into your software is always dangerous. The reason is that changing code will always bring risk. Better is to work with external config files so that configs are separated from logic. Another danger of hard-coding is the fact that code becomes harder to read as you are missing context. A typical example of this missing of context is the use of \"magic numbers\". These magic numbers are common in scripts where ad-hoc testing are performed. A magic number is a number that suddenly appears without any context. An example is shown in the code block below, where the number '52' seems to appear out of nowhere. ```python \"fisher-yates-magic.py\" def shuffle_items(items): import random for i in range(52): j = i + random.randrange(52) items[j], items[i] = items[i], items[j] return items In the above algorithm, we wanted to shuffle a deck of cards (52 cards). If we define this in a variable, the code suddenly becomes easier to read as we now have context (see below). ```python \"fisher-yates-shuffle-deck.py\" def shuffle_card_deck(items): import random card_deck_size = 52 for i in range(card_deck_size): j = i + random.randrange(card_deck_size) items[j], items[i] = items[i], items[j] return items Limit your line length. 'Code is read more often than it is written. Code should always be written in a way that promotes readability.' ~ Guido Van Rossum. This principle is mainly with the developer in mind to ease the readability of the code. A common rule is to limit the line width to 78-80 characters. This rule stems from the past where a computer screen could only contain 78 characters on a single line. More recent versions of this principle say to limit your code at 100-120 characters. Apart from \"screen width\" it should be noted that the optimal line length for a text to be \"readable\" is somewhere in the range 50-60 (some sources go as high as 75). Research found that longer line lengths may lead to reading mistakes (jumping back to the wrong line), resulting in a slower reading rate. This rule is for example implemented in the typesetting language LaTeX. But note that this rule is also used in printed books. Sign your code. This one is mostly about \"ownership and responsibility\". You should always strive to write code that you are proud of. If you are willing to actually \"sign\" your code, then this is an indication that it meets your personal quality standards. This ownership and responsibility should also be clear and open for everyone. Design to test. When writing code, always keep in mind that your code should be submitted to testing. A simple example of this is: always making sure that your code has a return value. If you want your code to be maintainable, this is a very strong principle. By adding unit tests to your code base, you can strongly increase the trust in your existing code. Fix the problem, not the blame. I first encountered this quote in the book \"The Pragmatic Programmer\". Afterwards I found out that this is actually a well-known Japanese proverb. The idea is that you should focus on the \"process\" rather than the \"people\". If you start the \"blame game\" you will end op with a broken system and a broken culture, while not fixing the problem. If you focus on the process, you will focus on \"best practices\" and the technical cause of the problem. When focussing on the process, you can work on self improvement and implement a \"growth mindset\" culture where you focus on improving the overall way of working. Explicit is better than implicit (keep it SMART). A principle that originates from the \"Zen of Python\" (PEP 20). In coding this is very important if you are thinking in terms of self-documentation and maintainability. Code where every line is completely clear is much easier to read than obscure code full of \"magic numbers\". However, in communication I am also a fan of this. Making things \"explicit\" makes sure that everyone is aligned and aware of what they are able to expect. When making proposals, making things explicit will allow others to either agree or to make a counter-proposal. In solution design (or project designs in general), making things explicit will also enable a better alignment. By adding specific milestones to the process you specify WHAT to deliver and WHEN to do it. In communication if you want things to be specific, the best acronym to remember is SMART. This means that every specific thing is Specific, Measurable, Acceptable, Realistic and Time-bound: Specific: Is there a clearly outlined goal that needs to be achieved? Measurable: What is the definition of done (DoD)? What metrics/KPIs can you use to measure the success? Acceptable: Are the goals also in line with what the customer/management is expecting? Realistic: Is this goal realistic? Time-bound: WHEN will these results be achieved? A clear committement on a deadline. Never make estimates that are too detailed. This idea is also neatly explained in the book: \"The Pragmatic Programmer\". There they also neatly explain how to generate a good estimate and update it using code. The general idea is that your estimate always (implicitly) reflects the accuracy in your estimate. For example, if you estimate in hours, the client will expect it to be correct up to several hours. If you on the other hand estimate in days, the expected accuracy is on the level of days. The longer your estimation, the less granular you should be with your initial estimate. Duration Quote estimate in 1 - 5 days (half) days 5-15 days days 3-8 weeks weeks 8-30 weeks months 30+ weeks think hard before giving an estimate. Silence is golden: the 4 second rule. Four seconds is all it takes for silence to get awkward. This is actually a very powerful tool in communication. By staying silent you give the other person the opportunity to also provide input. Often when brainstorming we have our biases and implicit assumptions. By allowing the other stakeholders to take the lead, we can see the problem from their perspective. If silence does not work, you can always follow it up with a set of open questions to check how the other stakeholders feel about the solution. Cunningham's law is your friend. Cunningham's law states: \"The best way to get the right answer (on the internet) is not to ask a question; it's to post the wrong answer.\". The underlying psychology is that people like to demonstrate when they know the answer. So if someone is giving you a hard time to get the solution of something, just say: \"I see, so it's actually <substitute wrong/random answer> \". You will see that most people can not resist to correct you then! This psychological effect is also very powerful when designing a solution with a client/stakeholder, or when working on a project. A good way to to use this effect during the design phase, or a technical discussion, is by saying something like: \"Am I understanding you correctly that: <Insert_assumption_that_has_been_made> \". If you are correct,the client will confirm, if you are wrong, you will immediately know. Never check your email in the morning. This is actually the title of the book that explains this principle. The general idea is to increase your focus time/productivity. The email (but also text messages etc) are a continuous flow of information. Because of our built-in FOMO (Fear Of Missing Out) we will always have the tendency to check these messages. The problem with this is that your workflow gets disturbed, making you loose your focus. The best thing to do is to explicitly \"plan\" when to read your email. This allows you to focus outside of these time-slots. Moreover, by planning your \"email-time\", you can also immediately react to the mail with your full focus.","title":"Coding Principles"},{"location":"Theory/deployment-strategies/","text":"Deployment Strategies General Software Deployment DevOps and CI/CD: best way to produce quality software. Dealing with legacy software: brown-out (temporarily reduce capacity). Machine Learning Deployment MLOps, same as DevOps, additionally now you also have the data that feeds your code (good versioning needed for reproducibility). Different deployment strategies: Shadow testing Canary release A/B testing Microservices strategies Main idea: ''From pets to cattle''. Serverless thinking -> Infrastructure as code. Popular pattern for introducing microservices: The \"strangler pattern\". Here you silently phase out... .","title":"Deployment Strategies"},{"location":"Theory/deployment-strategies/#deployment-strategies","text":"","title":"Deployment Strategies"},{"location":"Theory/deployment-strategies/#general-software-deployment","text":"DevOps and CI/CD: best way to produce quality software. Dealing with legacy software: brown-out (temporarily reduce capacity).","title":"General Software Deployment"},{"location":"Theory/deployment-strategies/#machine-learning-deployment","text":"MLOps, same as DevOps, additionally now you also have the data that feeds your code (good versioning needed for reproducibility). Different deployment strategies: Shadow testing Canary release A/B testing","title":"Machine Learning Deployment"},{"location":"Theory/deployment-strategies/#microservices-strategies","text":"Main idea: ''From pets to cattle''. Serverless thinking -> Infrastructure as code. Popular pattern for introducing microservices: The \"strangler pattern\". Here you silently phase out... .","title":"Microservices strategies"},{"location":"Theory/learning-and-certification/","text":"Learning and Certification Several useful resources for learning software development and AI. General advice on certification Typically when you are learning for a certification, the official docs will give you some indication on the skills that you need to master. In most cases, official learning material is advised/sold which can give you a quickstart. Nothing beats good exercise. In consultancy there is a big fetish for certification. But note that certification is not necessarily equal to mastery. I have personally worked with several consultants who had a lot of certificates but were unable to perform even basic tasks. A big reason for this is the fact that certification is a business aimed at selling certificates rather than experience. Using various exam dumps it is relatively easy to get any certification (see next subsection). My personal advice is to focus on the long-term goal and hone your skills. Sure if you are forced to get a certification, obtain it this way, but try to keep the big picture in mind. If you actually want to learn something, a certification is not the way to go. Exam dumps For common IT certifications, exam dumps are readily available. These exam dumps are a good test of your knowledge, but also a good source for learning. Typically around 80% of the practice exam dumps are also present on the actual exam 1 . This means that thoroughly studying an exam dump is usually sufficient to earn a certification. Since a good exam dump is almost a guarantee for a certification, there are a lot of places where you can buy these. Note however that there are also a lot of free resources for this available. Simply google \"free IT exam dumps\" and you will find a fairly extensive list. My personal favorites are exam topics and it exams . Coursera An online learning platform that has a lot of interesting courses in a wide variety of topics. This site is famous since they offer the Google certification training material, and since they also have the well received courses on Machine Learning of Andrew Ng (founder of Coursera). Each course offers a \"certificate of completion\". DataCamp An online learning platform focussed on data science. This site is focussed on learning data science via interactive coding in the browser. Note that the focus is solely on data science and machine learning, while the cost is comparable to coursera. This makes DataCamp rather expensive. Never the less it offers a good starting point for an aspiring Data Scientist. Each course offers a \"certificate of completion\". Pluralsight An online learning platform focussed on IT technology. Also a standard in IT training. Online roadmaps Another great way to learn general software development is by looking at online roadmaps. These can provide you with necessary guidance to get started and have a general background. A very complete source is the Developer Roadmaps website . Here you can find a high level roadmap for any type of ambition in software development. The website also contains several videos and resources to get you started. Simply googling \"Software Developer resources\" will also help a lot. A good source you will find quickly is the 10 free software development resources . Other learning resources Note that learning is never done. If you want to learn more about something, this is typically what I do: Looking at the university curriculum for software engineering (or the subject you are interested in). These courses typically have a list of resources that they use. These resources can then be used as study material. If you are research-minded you can even go one step further and look at the university's research groups: what topics are they exploring? A good review paper is golden in that case. Look at open job postings. The skills that the employers are demanding are usually the ones worth investing your time in as these will contribute to your employability. Typically these tools are also carefully chosen as a result of their personal experience/internal research. Study the questions on Stack Exchange. Typically these questions come from actual problems people are facing. Try to think about the provided solutions and what is happening. These kind of issues are also very good learning experiences. Join an online community. Here you will get triggered on a regular basis. And last but not least: Google is your friend! If you encounter something you do not understand, make a commitment to find out. It might be that you do it immediately, or that you write it down for a later time. Be aware for changes in exam content. The exam dumps CAN sometimes lag behind on the required exam content. Check the comments/recommendations of the exam dump for this, or look at the change/release dates of the exam content. \u21a9","title":"Learning and Certification"},{"location":"Theory/learning-and-certification/#learning-and-certification","text":"Several useful resources for learning software development and AI.","title":"Learning and Certification"},{"location":"Theory/learning-and-certification/#general-advice-on-certification","text":"Typically when you are learning for a certification, the official docs will give you some indication on the skills that you need to master. In most cases, official learning material is advised/sold which can give you a quickstart. Nothing beats good exercise. In consultancy there is a big fetish for certification. But note that certification is not necessarily equal to mastery. I have personally worked with several consultants who had a lot of certificates but were unable to perform even basic tasks. A big reason for this is the fact that certification is a business aimed at selling certificates rather than experience. Using various exam dumps it is relatively easy to get any certification (see next subsection). My personal advice is to focus on the long-term goal and hone your skills. Sure if you are forced to get a certification, obtain it this way, but try to keep the big picture in mind. If you actually want to learn something, a certification is not the way to go.","title":"General advice on certification"},{"location":"Theory/learning-and-certification/#exam-dumps","text":"For common IT certifications, exam dumps are readily available. These exam dumps are a good test of your knowledge, but also a good source for learning. Typically around 80% of the practice exam dumps are also present on the actual exam 1 . This means that thoroughly studying an exam dump is usually sufficient to earn a certification. Since a good exam dump is almost a guarantee for a certification, there are a lot of places where you can buy these. Note however that there are also a lot of free resources for this available. Simply google \"free IT exam dumps\" and you will find a fairly extensive list. My personal favorites are exam topics and it exams .","title":"Exam dumps"},{"location":"Theory/learning-and-certification/#coursera","text":"An online learning platform that has a lot of interesting courses in a wide variety of topics. This site is famous since they offer the Google certification training material, and since they also have the well received courses on Machine Learning of Andrew Ng (founder of Coursera). Each course offers a \"certificate of completion\".","title":"Coursera"},{"location":"Theory/learning-and-certification/#datacamp","text":"An online learning platform focussed on data science. This site is focussed on learning data science via interactive coding in the browser. Note that the focus is solely on data science and machine learning, while the cost is comparable to coursera. This makes DataCamp rather expensive. Never the less it offers a good starting point for an aspiring Data Scientist. Each course offers a \"certificate of completion\".","title":"DataCamp"},{"location":"Theory/learning-and-certification/#pluralsight","text":"An online learning platform focussed on IT technology. Also a standard in IT training.","title":"Pluralsight"},{"location":"Theory/learning-and-certification/#online-roadmaps","text":"Another great way to learn general software development is by looking at online roadmaps. These can provide you with necessary guidance to get started and have a general background. A very complete source is the Developer Roadmaps website . Here you can find a high level roadmap for any type of ambition in software development. The website also contains several videos and resources to get you started. Simply googling \"Software Developer resources\" will also help a lot. A good source you will find quickly is the 10 free software development resources .","title":"Online roadmaps"},{"location":"Theory/learning-and-certification/#other-learning-resources","text":"Note that learning is never done. If you want to learn more about something, this is typically what I do: Looking at the university curriculum for software engineering (or the subject you are interested in). These courses typically have a list of resources that they use. These resources can then be used as study material. If you are research-minded you can even go one step further and look at the university's research groups: what topics are they exploring? A good review paper is golden in that case. Look at open job postings. The skills that the employers are demanding are usually the ones worth investing your time in as these will contribute to your employability. Typically these tools are also carefully chosen as a result of their personal experience/internal research. Study the questions on Stack Exchange. Typically these questions come from actual problems people are facing. Try to think about the provided solutions and what is happening. These kind of issues are also very good learning experiences. Join an online community. Here you will get triggered on a regular basis. And last but not least: Google is your friend! If you encounter something you do not understand, make a commitment to find out. It might be that you do it immediately, or that you write it down for a later time. Be aware for changes in exam content. The exam dumps CAN sometimes lag behind on the required exam content. Check the comments/recommendations of the exam dump for this, or look at the change/release dates of the exam content. \u21a9","title":"Other learning resources"},{"location":"Theory/mlops/","text":"Machine Learning Operations Machine Learning Operations (MLOps) is a set of practices used to deploy and manage ML models in production. DevOps: Classic software development Works on the CI/CD principle. Many important concepts here! See QA for testing examples. Within the DevOps way of working we have two important pillars known as CI/CD: Continuous Integration (CI): Rules to work with a scalable code base. Continuous Deployment/Delivery (CD): Automated deployment of the code. Continuous Integration (CI) Continuous Deployment/Delivery (CD) MLOps MLOps is the extension suitable for Machine Learning models (see challenges). In order to deal with these additional difficulties, two additional pi:lars are needed. For MLOps we therefore work with the following components: Continuous Integration (CI): Rules to work with a scalable code base. Continuous Deployment/Delivery (CD): Automated deployment of the code. Continuous Monitoring (CM): Automated and structured follow up of model performance. Continuous Training (CT): Automated retraining of ML models once a certain criterion is met (e.g. quality metric, or number of new data points, or just a simple time-based metric). Challenges for machine learning Classic code development is relatively \"simple\". You write your code and deploy it to run somewhere. With Machine Learning you have the following additional difficulties: The system is alive: With ML you are modeling a real-world system. This means that the underlying behavior (and hence data distributions) can change while the code is running. ML models are not just code. It's the result of an optimization procedure, based on a given training dataset. CI/CD for Machine Learning Not much difference. Important additional checks now are: Fairness? Does the AI model not discriminate? Common sense? Does the AI model make logical predictions? Continuous monitoring (CM) With this part we solve the first issue: tracking the changes in underlying behavior. Continuous Training (CT) With this part we respond to the results of CM, or based on fixed rules. Important to note is that ML models are more than just code. We also need to track: Datasets: Which version was used? Underlying model that was used and the optimization procedure. The obtained hyperparameters. The idea with continuos training is that you keep a track record of your deployed/trained models. This can then be used to easily roll back to a previous version of the model if needed. ModelOps","title":"Machine Learning Operations"},{"location":"Theory/mlops/#machine-learning-operations","text":"Machine Learning Operations (MLOps) is a set of practices used to deploy and manage ML models in production.","title":"Machine Learning Operations"},{"location":"Theory/mlops/#devops-classic-software-development","text":"Works on the CI/CD principle. Many important concepts here! See QA for testing examples. Within the DevOps way of working we have two important pillars known as CI/CD: Continuous Integration (CI): Rules to work with a scalable code base. Continuous Deployment/Delivery (CD): Automated deployment of the code.","title":"DevOps: Classic software development"},{"location":"Theory/mlops/#continuous-integration-ci","text":"","title":"Continuous Integration (CI)"},{"location":"Theory/mlops/#continuous-deploymentdelivery-cd","text":"","title":"Continuous Deployment/Delivery (CD)"},{"location":"Theory/mlops/#mlops","text":"MLOps is the extension suitable for Machine Learning models (see challenges). In order to deal with these additional difficulties, two additional pi:lars are needed. For MLOps we therefore work with the following components: Continuous Integration (CI): Rules to work with a scalable code base. Continuous Deployment/Delivery (CD): Automated deployment of the code. Continuous Monitoring (CM): Automated and structured follow up of model performance. Continuous Training (CT): Automated retraining of ML models once a certain criterion is met (e.g. quality metric, or number of new data points, or just a simple time-based metric).","title":"MLOps"},{"location":"Theory/mlops/#challenges-for-machine-learning","text":"Classic code development is relatively \"simple\". You write your code and deploy it to run somewhere. With Machine Learning you have the following additional difficulties: The system is alive: With ML you are modeling a real-world system. This means that the underlying behavior (and hence data distributions) can change while the code is running. ML models are not just code. It's the result of an optimization procedure, based on a given training dataset.","title":"Challenges for machine learning"},{"location":"Theory/mlops/#cicd-for-machine-learning","text":"Not much difference. Important additional checks now are: Fairness? Does the AI model not discriminate? Common sense? Does the AI model make logical predictions?","title":"CI/CD for Machine Learning"},{"location":"Theory/mlops/#continuous-monitoring-cm","text":"With this part we solve the first issue: tracking the changes in underlying behavior.","title":"Continuous monitoring (CM)"},{"location":"Theory/mlops/#continuous-training-ct","text":"With this part we respond to the results of CM, or based on fixed rules. Important to note is that ML models are more than just code. We also need to track: Datasets: Which version was used? Underlying model that was used and the optimization procedure. The obtained hyperparameters. The idea with continuos training is that you keep a track record of your deployed/trained models. This can then be used to easily roll back to a previous version of the model if needed.","title":"Continuous Training (CT)"},{"location":"Theory/mlops/#modelops","text":"","title":"ModelOps"},{"location":"Theory/naming-conventions/","text":"Naming conventions There are different types of conventions out there for file naming. Here we discuss the most common cases. Case Styles The case styles are the most common ways to combine multiple words into a single string. The reason to do this is because of the fact that programs typically reserve the space character for special purposes. For example it is in most programming languages not possible to define a variable name that contains a space. The most common case styles that you can encounter are: snake_case : Combine words by replacing all spaces by underscores '_'. All words are usually lower case. Typically used for declaring constants in many languages. The ALL_CAPS variant also exists, where it is typically used to denote constants in the environment or global variables. kebab-case : Combine words by replacing all spaces by dashes '-'. All words are lower case. Typically used in URLs. Note that this is also very useful to define folder structures and filenames. When using the dash - to split up words, you can also use REGEX expressions to do pattern matching as all words will be recognized as separate entities (with underscores this is not the case). camelCase : Combine words by removing all spaces and capitalizing all words except the first word. In many languages, this casing is used to define variables (e.g. isActive ). PascalCase : Combine words by removing all spaces and capitalizing all words. Typically used to define classes in most programming languages. Consistency is most important. Which casing you choose is up to you. The most important thing is to be consistent. For python, a big help are the PEP8 guidelines.","title":"Naming conventions"},{"location":"Theory/naming-conventions/#naming-conventions","text":"There are different types of conventions out there for file naming. Here we discuss the most common cases.","title":"Naming conventions"},{"location":"Theory/naming-conventions/#case-styles","text":"The case styles are the most common ways to combine multiple words into a single string. The reason to do this is because of the fact that programs typically reserve the space character for special purposes. For example it is in most programming languages not possible to define a variable name that contains a space. The most common case styles that you can encounter are: snake_case : Combine words by replacing all spaces by underscores '_'. All words are usually lower case. Typically used for declaring constants in many languages. The ALL_CAPS variant also exists, where it is typically used to denote constants in the environment or global variables. kebab-case : Combine words by replacing all spaces by dashes '-'. All words are lower case. Typically used in URLs. Note that this is also very useful to define folder structures and filenames. When using the dash - to split up words, you can also use REGEX expressions to do pattern matching as all words will be recognized as separate entities (with underscores this is not the case). camelCase : Combine words by removing all spaces and capitalizing all words except the first word. In many languages, this casing is used to define variables (e.g. isActive ). PascalCase : Combine words by removing all spaces and capitalizing all words. Typically used to define classes in most programming languages. Consistency is most important. Which casing you choose is up to you. The most important thing is to be consistent. For python, a big help are the PEP8 guidelines.","title":"Case Styles"},{"location":"Theory/quality-assurance/","text":"Quality Assurance Quality Assurance, or QA for short, is the set of practices that safeguard the quality of a given solution. In order to maximize the gains from QA, this should be automated as much as possible in a Continuous Integration (CI) loop within the DevOps cycle. A typical QA flow Typically in a QA flow a typical Quality Control (QC) flow is followed: first test before deploying to production. In the most ideal case, you have the DTAP environments: Development: In this environment the actual code is developed. Developers can freely experiment without worrying about production. All changes of the development environment are combined using the Continuous Integration (CI) best practices. Testing: From development, code can be automatically deployed using the Continuous Delevery/Deployment (CD) best practices. On this server the different tests can be ran. Acceptance: If the tests are approved, the code can be automatically deployed to the acceptance server. This environment is an exact copy of the production environment and ideal for performing integration testing. This environment can be use to showcase new features to business and end-users and allow them to test these. Production: If all code changes are approved, the acceptance code can again be automatically deployed to production. The key words in all stages are automated deployment , at each step we should avoid manual intervention. Note that for smaller projects, keeping up 4 environments if often too costly. You should always strive to have at least a separate development and production environment if you want to avoid downtimes. Testing/acceptance can then be (partially) solved by setting up a good CI scheme including automated testing. Automated Testing Techniques The \"automated testing techniques\" are tools that are simple and can be included in the CD loop without too much effort. Linting A linter is a static code analysis tool. This tool will do a full scan of the source files and try to detect possible syntax errors without running/compiling the code . Next to detecting possible syntax errors, some linters will also enforce/detect code styling conventions (e.g. flask8 will enforce the PEP8 coding conventions in python). Unit testing A unit test is a technique where isolated parts of the code are tested on predefined test cases. By adding unit tests to your CD process, you increase the trust in your codebase as parts will get automatically tested. The amount of code that is submitted to unit testing is called the \"code coverage\", the higher the better. Note that code coverage and amount of unit tests that pass are typical metrics to report on CD systems (e.g. GitHub). Good unit tests to run are: Tests you expect to work (typical uses) Tests you expect to fail (cases that should fail) Edge cases (cases that only occur in very extreme cases) Manual Testing Techniques The tests here are typically ran either manual during a development cycle, or after deployment on a dedicated test/acceptance server. In the latter case, these flows are often scheduled. Integration testing When integration testing, all pieces of the code and ran together. By integration testing you can get an idea on how the code performs and works together when the software is ran end-to-end. Typically when integration testing, you might want to look at the following metrics: Timing: How much time does the code need to run. This might for example be relevant for API endpoints. Resource usage: Can the code comfortably run on the given resource? What is de CPU/Memory usage? Regression testing Tests to check if software is still correctly functioning after a change has occurred. Load testing With load testing, you will test how the code works with multiple incoming requests. In this type of tests, the code is ran multiple times. By increasing the number of runs/requests, one can see how the code performance remains under different loads. This is a special case of integration testing which is typically used to test API endpoints. Sanity and Smoke testing In this type of testing, you try typical cases in the code and see if the code behaves as you expect it to behave. With \"sanity testing\" you apply these tests to new software features to see if these behave as expected (and as logical). This can either be a newly introduced feature or a bug that had to be fixed. With \"smoke testig\" you apply these tests to existing software features to see if critical parts of the software are working fine. These tests will typically be promoted to unit tests.","title":"Quality Assurance"},{"location":"Theory/quality-assurance/#quality-assurance","text":"Quality Assurance, or QA for short, is the set of practices that safeguard the quality of a given solution. In order to maximize the gains from QA, this should be automated as much as possible in a Continuous Integration (CI) loop within the DevOps cycle.","title":"Quality Assurance"},{"location":"Theory/quality-assurance/#a-typical-qa-flow","text":"Typically in a QA flow a typical Quality Control (QC) flow is followed: first test before deploying to production. In the most ideal case, you have the DTAP environments: Development: In this environment the actual code is developed. Developers can freely experiment without worrying about production. All changes of the development environment are combined using the Continuous Integration (CI) best practices. Testing: From development, code can be automatically deployed using the Continuous Delevery/Deployment (CD) best practices. On this server the different tests can be ran. Acceptance: If the tests are approved, the code can be automatically deployed to the acceptance server. This environment is an exact copy of the production environment and ideal for performing integration testing. This environment can be use to showcase new features to business and end-users and allow them to test these. Production: If all code changes are approved, the acceptance code can again be automatically deployed to production. The key words in all stages are automated deployment , at each step we should avoid manual intervention. Note that for smaller projects, keeping up 4 environments if often too costly. You should always strive to have at least a separate development and production environment if you want to avoid downtimes. Testing/acceptance can then be (partially) solved by setting up a good CI scheme including automated testing.","title":"A typical QA flow"},{"location":"Theory/quality-assurance/#automated-testing-techniques","text":"The \"automated testing techniques\" are tools that are simple and can be included in the CD loop without too much effort.","title":"Automated Testing Techniques"},{"location":"Theory/quality-assurance/#linting","text":"A linter is a static code analysis tool. This tool will do a full scan of the source files and try to detect possible syntax errors without running/compiling the code . Next to detecting possible syntax errors, some linters will also enforce/detect code styling conventions (e.g. flask8 will enforce the PEP8 coding conventions in python).","title":"Linting"},{"location":"Theory/quality-assurance/#unit-testing","text":"A unit test is a technique where isolated parts of the code are tested on predefined test cases. By adding unit tests to your CD process, you increase the trust in your codebase as parts will get automatically tested. The amount of code that is submitted to unit testing is called the \"code coverage\", the higher the better. Note that code coverage and amount of unit tests that pass are typical metrics to report on CD systems (e.g. GitHub). Good unit tests to run are: Tests you expect to work (typical uses) Tests you expect to fail (cases that should fail) Edge cases (cases that only occur in very extreme cases)","title":"Unit testing"},{"location":"Theory/quality-assurance/#manual-testing-techniques","text":"The tests here are typically ran either manual during a development cycle, or after deployment on a dedicated test/acceptance server. In the latter case, these flows are often scheduled.","title":"Manual Testing Techniques"},{"location":"Theory/quality-assurance/#integration-testing","text":"When integration testing, all pieces of the code and ran together. By integration testing you can get an idea on how the code performs and works together when the software is ran end-to-end. Typically when integration testing, you might want to look at the following metrics: Timing: How much time does the code need to run. This might for example be relevant for API endpoints. Resource usage: Can the code comfortably run on the given resource? What is de CPU/Memory usage?","title":"Integration testing"},{"location":"Theory/quality-assurance/#regression-testing","text":"Tests to check if software is still correctly functioning after a change has occurred.","title":"Regression testing"},{"location":"Theory/quality-assurance/#load-testing","text":"With load testing, you will test how the code works with multiple incoming requests. In this type of tests, the code is ran multiple times. By increasing the number of runs/requests, one can see how the code performance remains under different loads. This is a special case of integration testing which is typically used to test API endpoints.","title":"Load testing"},{"location":"Theory/quality-assurance/#sanity-and-smoke-testing","text":"In this type of testing, you try typical cases in the code and see if the code behaves as you expect it to behave. With \"sanity testing\" you apply these tests to new software features to see if these behave as expected (and as logical). This can either be a newly introduced feature or a bug that had to be fixed. With \"smoke testig\" you apply these tests to existing software features to see if critical parts of the software are working fine. These tests will typically be promoted to unit tests.","title":"Sanity and Smoke testing"},{"location":"Theory/software-engineering/","text":"Software Engineering Gitflow A commonly used branching structure for git projects. In this model we have two long-living branches: \"main\" and \"develop\". Developments on the software then happen on the feature branches, labeled \"feature/ \". Bugfixes/hotfixes can follow a similar pattern, these can be pushed to braches labeled, \"bugfix/ \". By working with the two long-living branches, we can easily set up the apropriate CI/CD flows. In general one can not directly commit code to 'develop' and 'main'. Pushing code to 'develop' will trigger a deploy to the 'test' environment. Once the code is accepted, a pull from 'develop' to 'main' will trigger a deploy to production. Place your git repositories in a fixed folder. A common choice is to place your git repositories in the C:\\users\\<username>\\git directory. If you are developing on a WSL setup, you can then easily access this repository via mnt/c/users/<username>/git path. Dependency management When developing software, always make sure that you have the correct environment set up. A part of this is nowadays fixed by working with Docker containers. But there is also an important part in getting the correct package versions. With python you can write away your environment using the pip freeze command, which will list all dependencies. The makefile Originated from C and C++ to compile code. Is available by default in Linux and OSX by using the make command. A neat example for python is given below. SHELL := /bin/bash VENV = ./.venv PYTHON = $( VENV ) /bin/python3 PIP = $( VENV ) /bin/pip # .PHONY: run clean setup : requirements . txt python -m venv $( VENV ) $( PIP ) install -r requirements.txt run : source ./.venv/bin/activate echo \"No scripts yet!\" clean : deactivate rm -rf __pycache__ rm -rf $( VENV )","title":"Software Engineering"},{"location":"Theory/software-engineering/#software-engineering","text":"","title":"Software Engineering"},{"location":"Theory/software-engineering/#gitflow","text":"A commonly used branching structure for git projects. In this model we have two long-living branches: \"main\" and \"develop\". Developments on the software then happen on the feature branches, labeled \"feature/ \". Bugfixes/hotfixes can follow a similar pattern, these can be pushed to braches labeled, \"bugfix/ \". By working with the two long-living branches, we can easily set up the apropriate CI/CD flows. In general one can not directly commit code to 'develop' and 'main'. Pushing code to 'develop' will trigger a deploy to the 'test' environment. Once the code is accepted, a pull from 'develop' to 'main' will trigger a deploy to production. Place your git repositories in a fixed folder. A common choice is to place your git repositories in the C:\\users\\<username>\\git directory. If you are developing on a WSL setup, you can then easily access this repository via mnt/c/users/<username>/git path.","title":"Gitflow"},{"location":"Theory/software-engineering/#dependency-management","text":"When developing software, always make sure that you have the correct environment set up. A part of this is nowadays fixed by working with Docker containers. But there is also an important part in getting the correct package versions. With python you can write away your environment using the pip freeze command, which will list all dependencies.","title":"Dependency management"},{"location":"Theory/software-engineering/#the-makefile","text":"Originated from C and C++ to compile code. Is available by default in Linux and OSX by using the make command. A neat example for python is given below. SHELL := /bin/bash VENV = ./.venv PYTHON = $( VENV ) /bin/python3 PIP = $( VENV ) /bin/pip # .PHONY: run clean setup : requirements . txt python -m venv $( VENV ) $( PIP ) install -r requirements.txt run : source ./.venv/bin/activate echo \"No scripts yet!\" clean : deactivate rm -rf __pycache__ rm -rf $( VENV )","title":"The makefile"},{"location":"Theory/solution-development/","text":"Solution Development In this section general principles are discussed for architecting/designing general (IT) solutions. Important is to always keep the business and usability in mind. Stages of Solution Development Developing a solution happens in four phases: Pilot/PoC: A 2-3 people work on it. Main stakeholders aware. See if solution works. The main goal of this phase is to ensure key functionalities can be met and the right stakeholders are aware of the project. End result is buy-in from management and feasibility check. Minimal Viable Product (MVP): A first release for a broader audience. pre production production Requirements Engineering","title":"Solution Development"},{"location":"Theory/solution-development/#solution-development","text":"In this section general principles are discussed for architecting/designing general (IT) solutions. Important is to always keep the business and usability in mind.","title":"Solution Development"},{"location":"Theory/solution-development/#stages-of-solution-development","text":"Developing a solution happens in four phases: Pilot/PoC: A 2-3 people work on it. Main stakeholders aware. See if solution works. The main goal of this phase is to ensure key functionalities can be met and the right stakeholders are aware of the project. End result is buy-in from management and feasibility check. Minimal Viable Product (MVP): A first release for a broader audience. pre production production","title":"Stages of Solution Development"},{"location":"Theory/solution-development/#requirements-engineering","text":"","title":"Requirements Engineering"},{"location":"Theory/windows-subsystem-for-linux/","text":"Windows Subsystem for Linux Since windows 10, the development experience has drastically improved by the introduction of the Windows Subsystem for Linux (WSL). This allows you to simultaneously run Windows and Linux on the same machine. This is very useful for software developers since linux is very common there as it makes developing a lot easier. With the introduction of WSL, Windows became a good competitor to Linux and Mac for developers once more. Recommended setup for software engineering This setup is also recommended by Microsoft! High level the steps are the following: Install the Windows Subsystem for Linux (activate via the settings). Set up your IDE (recommended VSCode) in Windows and connect to it via the terminal. For VSCode the \"Remove WSL\" extension allows you to seamlessly connect to your Linux distribution and connect to the python environments there. Install all of your packages in the Linux distribution. Note that you will have two separated file systems: one on Windows and one on Linux. The general recommendation is to install your packages on your Linux distribution. The reason is that Linux is used more often to develop (and deploy) these packages, so it is expected that the development experience is much smoother on a linux OS. python other packages For python the recommended packages are: python, pip and venv. To get started you first run sudo apt update && sudo apt upgrade . This will make sure your Linux distribution is updated to its latest version. Placeholder for other software. Mounting your Windows drive Your Windows C-drive is mounted by default on the /mnt/c location. It is possible to set up the default startup location to C:\\Users\\<username>\\ . Setting up docker with WSL containers When Docker Desktop is installed, you can select any of your WSL2 containers from within the software. Using multiple Linux distros Note that Linux comes in a wide variety: Ubuntu: The default go-to for every developer. Kali: Distro aimed at pentesters. Debian: ???","title":"Windows Subsystem for Linux"},{"location":"Theory/windows-subsystem-for-linux/#windows-subsystem-for-linux","text":"Since windows 10, the development experience has drastically improved by the introduction of the Windows Subsystem for Linux (WSL). This allows you to simultaneously run Windows and Linux on the same machine. This is very useful for software developers since linux is very common there as it makes developing a lot easier. With the introduction of WSL, Windows became a good competitor to Linux and Mac for developers once more.","title":"Windows Subsystem for Linux"},{"location":"Theory/windows-subsystem-for-linux/#recommended-setup-for-software-engineering","text":"This setup is also recommended by Microsoft! High level the steps are the following: Install the Windows Subsystem for Linux (activate via the settings). Set up your IDE (recommended VSCode) in Windows and connect to it via the terminal. For VSCode the \"Remove WSL\" extension allows you to seamlessly connect to your Linux distribution and connect to the python environments there. Install all of your packages in the Linux distribution. Note that you will have two separated file systems: one on Windows and one on Linux. The general recommendation is to install your packages on your Linux distribution. The reason is that Linux is used more often to develop (and deploy) these packages, so it is expected that the development experience is much smoother on a linux OS. python other packages For python the recommended packages are: python, pip and venv. To get started you first run sudo apt update && sudo apt upgrade . This will make sure your Linux distribution is updated to its latest version. Placeholder for other software.","title":"Recommended setup for software engineering"},{"location":"Theory/windows-subsystem-for-linux/#mounting-your-windows-drive","text":"Your Windows C-drive is mounted by default on the /mnt/c location. It is possible to set up the default startup location to C:\\Users\\<username>\\ .","title":"Mounting your Windows drive"},{"location":"Theory/windows-subsystem-for-linux/#setting-up-docker-with-wsl-containers","text":"When Docker Desktop is installed, you can select any of your WSL2 containers from within the software.","title":"Setting up docker with WSL containers"},{"location":"Theory/windows-subsystem-for-linux/#using-multiple-linux-distros","text":"Note that Linux comes in a wide variety: Ubuntu: The default go-to for every developer. Kali: Distro aimed at pentesters. Debian: ???","title":"Using multiple Linux distros"},{"location":"Tools/","text":"Tools A collection of tools that are useful for software development.","title":"Tools"},{"location":"Tools/#tools","text":"A collection of tools that are useful for software development.","title":"Tools"},{"location":"Tools/api/","text":"API Postman Powerful tool for testing APIs Flask Python package for building APIs","title":"API"},{"location":"Tools/api/#api","text":"","title":"API"},{"location":"Tools/api/#postman","text":"Powerful tool for testing APIs","title":"Postman"},{"location":"Tools/api/#flask","text":"Python package for building APIs","title":"Flask"},{"location":"Tools/connectivity/","text":"Connectivity FileZilla A good tool to connect to FTP (File Transfer Protocol) servers.","title":"Connectivity"},{"location":"Tools/connectivity/#connectivity","text":"","title":"Connectivity"},{"location":"Tools/connectivity/#filezilla","text":"A good tool to connect to FTP (File Transfer Protocol) servers.","title":"FileZilla"},{"location":"Tools/data/","text":"Data 7zip Not a database, but a good way of working with compressed data. With 7Zip you have additional data formats that can be unpacked. DBeaver An open-source ODBC that has a lot of connectors. SQL Server Management Studio (SSMS) Specifically taylored to SQL databases.","title":"Data"},{"location":"Tools/data/#data","text":"","title":"Data"},{"location":"Tools/data/#7zip","text":"Not a database, but a good way of working with compressed data. With 7Zip you have additional data formats that can be unpacked.","title":"7zip"},{"location":"Tools/data/#dbeaver","text":"An open-source ODBC that has a lot of connectors.","title":"DBeaver"},{"location":"Tools/data/#sql-server-management-studio-ssms","text":"Specifically taylored to SQL databases.","title":"SQL Server Management Studio (SSMS)"},{"location":"Tools/design-and-documentation/","text":"Design and Documentation A good standard is to use open-source technology as much as possible. This will make your documentation portable. For text, a good standard is 'markdown'. notepad++ A standard liteweight text editor. The text editor can be expanded with various plugins to improve the user experience. Interesting plugins: JSON viewer: Allows you to easily rearrange and explore JSON files. Better Multi Selection: Using the alt-key you can select and edit multiple lines at once. MIME tools: Allows you to work with BASE64 encoded files. Compare: Allows you to easily compare two documents side-by-side. draw.io Free drawing tool flaticon Good site for free icons VSCode Has a good built-in markdown editor with the possibility to preview the output. Also mentioned in software-development tools. TexStudio Mathematical editor based on LaTeX. TexLive Package manager for LaTeX packages. An alternative is MikTex which works excellent for windows. The main advantage of using TexLive is that it will work the same on all operating systems (it was designed that way). mkdocs Python package that allows you to easily create documentation as a static website using markdown files.","title":"Design and Documentation"},{"location":"Tools/design-and-documentation/#design-and-documentation","text":"A good standard is to use open-source technology as much as possible. This will make your documentation portable. For text, a good standard is 'markdown'.","title":"Design and Documentation"},{"location":"Tools/design-and-documentation/#notepad","text":"A standard liteweight text editor. The text editor can be expanded with various plugins to improve the user experience. Interesting plugins: JSON viewer: Allows you to easily rearrange and explore JSON files. Better Multi Selection: Using the alt-key you can select and edit multiple lines at once. MIME tools: Allows you to work with BASE64 encoded files. Compare: Allows you to easily compare two documents side-by-side.","title":"notepad++"},{"location":"Tools/design-and-documentation/#drawio","text":"Free drawing tool","title":"draw.io"},{"location":"Tools/design-and-documentation/#flaticon","text":"Good site for free icons","title":"flaticon"},{"location":"Tools/design-and-documentation/#vscode","text":"Has a good built-in markdown editor with the possibility to preview the output. Also mentioned in software-development tools.","title":"VSCode"},{"location":"Tools/design-and-documentation/#texstudio","text":"Mathematical editor based on LaTeX.","title":"TexStudio"},{"location":"Tools/design-and-documentation/#texlive","text":"Package manager for LaTeX packages. An alternative is MikTex which works excellent for windows. The main advantage of using TexLive is that it will work the same on all operating systems (it was designed that way).","title":"TexLive"},{"location":"Tools/design-and-documentation/#mkdocs","text":"Python package that allows you to easily create documentation as a static website using markdown files.","title":"mkdocs"},{"location":"Tools/package-managers/","text":"Package Managers Chocolatey Chocolatey (or choco for short) is a software package manager for Windows. Aims to add similar install features as you natively have in Linux. Automatically deploy new system with choco. Chocolatey has a very large repository of available packages. This allows you to script the installation of all of your software in one single script that does the installation via choco.","title":"Package Managers"},{"location":"Tools/package-managers/#package-managers","text":"","title":"Package Managers"},{"location":"Tools/package-managers/#chocolatey","text":"Chocolatey (or choco for short) is a software package manager for Windows. Aims to add similar install features as you natively have in Linux. Automatically deploy new system with choco. Chocolatey has a very large repository of available packages. This allows you to script the installation of all of your software in one single script that does the installation via choco.","title":"Chocolatey"},{"location":"Tools/project-organization/","text":"Project Organization Microsoft ToDo Note that this also neatly integrates with Microsoft Outlook. Emails that are \"flagged\" are automatically added to the to-do list. Trello OneNote Working Hours Good to track time.","title":"Project Organization"},{"location":"Tools/project-organization/#project-organization","text":"","title":"Project Organization"},{"location":"Tools/project-organization/#microsoft-todo","text":"Note that this also neatly integrates with Microsoft Outlook. Emails that are \"flagged\" are automatically added to the to-do list.","title":"Microsoft ToDo"},{"location":"Tools/project-organization/#trello","text":"","title":"Trello"},{"location":"Tools/project-organization/#onenote","text":"","title":"OneNote"},{"location":"Tools/project-organization/#working-hours","text":"Good to track time.","title":"Working Hours"},{"location":"Tools/software-development/","text":"Integrated Development Environments (IDEs) Anaconda Package manager for Anaconda. Also installs python. Node.JS and npm Install via the Node.JS website. VSCode Microsoft editor. Useful packages: One Dark Pro: The dark theme of Atom Gitlens: More easily browse git branches Git & GitHub extension pack: Full set of github packages (see for interesting packages there ...) Output Colorizer: Colorizing of log files. Live Share: Share your code interactively. Path Intellisense: Autocompletion for path names in code Settings Sync: Synchronize your VSCode settings via GitHub automatically. Better Comments: colorization for comments. Bracket pair colorizer 2: Coloring of bracket pairs in code. github markdown preview: Overrides the markdown preview with the version of github. Markdown all in one: Tools to simplify writing markdown. Code Spell Checker: Checks spelling of your code. Random Everything: Generates random data automatically. CodeTour: Record and playback guided tours of code bases. Debug Visualizer: Visual watch window on the data structures during the debug flow. Python: Python language support (allows you to find available python environments) Jupyter: Allows notebook processing in VSCode markdownlint: Linting for markdown files Peacock: Allows dynamic coloring of VSCode workspace Azure Tools: Installs all required Azure packages Azure IoT Tools: Installs packages to work with IoT devices. Atom Alternative editor. Windows Subsystem for Linux (WSL) When working on a Windows machine, a good practice is to set up and enable WSL as soon as possible. Note that windows requires you to both enable the WSL AND install a linux operating system (via the Windows store). The recommended operating system to start with is Ubuntu. This is the most popular choice. Terminal The Windows app \"terminal\" is your one-stop shop for all terminal activity. By default this supports classic \"command prompt\" and PowerShell. But other operating systems can be added. For example is you activate WSL, you will also have the option to run bash commands on the linux kernel in the subsystem. This is very useful to harvest the power of linux on the command line. Packages: Az: Azure module for powershell","title":"Integrated Development Environments (IDEs)"},{"location":"Tools/software-development/#integrated-development-environments-ides","text":"","title":"Integrated Development Environments (IDEs)"},{"location":"Tools/software-development/#anaconda","text":"Package manager for Anaconda. Also installs python.","title":"Anaconda"},{"location":"Tools/software-development/#nodejs-and-npm","text":"Install via the Node.JS website.","title":"Node.JS and npm"},{"location":"Tools/software-development/#vscode","text":"Microsoft editor. Useful packages: One Dark Pro: The dark theme of Atom Gitlens: More easily browse git branches Git & GitHub extension pack: Full set of github packages (see for interesting packages there ...) Output Colorizer: Colorizing of log files. Live Share: Share your code interactively. Path Intellisense: Autocompletion for path names in code Settings Sync: Synchronize your VSCode settings via GitHub automatically. Better Comments: colorization for comments. Bracket pair colorizer 2: Coloring of bracket pairs in code. github markdown preview: Overrides the markdown preview with the version of github. Markdown all in one: Tools to simplify writing markdown. Code Spell Checker: Checks spelling of your code. Random Everything: Generates random data automatically. CodeTour: Record and playback guided tours of code bases. Debug Visualizer: Visual watch window on the data structures during the debug flow. Python: Python language support (allows you to find available python environments) Jupyter: Allows notebook processing in VSCode markdownlint: Linting for markdown files Peacock: Allows dynamic coloring of VSCode workspace Azure Tools: Installs all required Azure packages Azure IoT Tools: Installs packages to work with IoT devices.","title":"VSCode"},{"location":"Tools/software-development/#atom","text":"Alternative editor.","title":"Atom"},{"location":"Tools/software-development/#windows-subsystem-for-linux-wsl","text":"When working on a Windows machine, a good practice is to set up and enable WSL as soon as possible. Note that windows requires you to both enable the WSL AND install a linux operating system (via the Windows store). The recommended operating system to start with is Ubuntu. This is the most popular choice.","title":"Windows Subsystem for Linux (WSL)"},{"location":"Tools/software-development/#terminal","text":"The Windows app \"terminal\" is your one-stop shop for all terminal activity. By default this supports classic \"command prompt\" and PowerShell. But other operating systems can be added. For example is you activate WSL, you will also have the option to run bash commands on the linux kernel in the subsystem. This is very useful to harvest the power of linux on the command line. Packages: Az: Azure module for powershell","title":"Terminal"},{"location":"Tools/visualization/","text":"Visualization Grafana PowerBI (Free, but not hosted online then)","title":"Visualization"},{"location":"Tools/visualization/#visualization","text":"","title":"Visualization"},{"location":"Tools/visualization/#grafana","text":"","title":"Grafana"},{"location":"Tools/visualization/#powerbi","text":"(Free, but not hosted online then)","title":"PowerBI"}]}